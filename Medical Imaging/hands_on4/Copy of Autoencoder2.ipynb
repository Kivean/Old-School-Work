{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Autoencoder2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yxmW_YF8UDvK","colab_type":"code","outputId":"6f637631-6764-4cfe-8697-a48abd70d31d","executionInfo":{"status":"ok","timestamp":1572289373449,"user_tz":240,"elapsed":390,"user":{"displayName":"xd xd","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAgTHZWotfUszvYUVAYAD0KBsJyLtefel7POB-l=s64","userId":"07693667503497446418"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#!/usr/bin/env python\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import time\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","from skimage.transform import radon, iradon\n","from skimage.measure import compare_ssim\n","import math\n","from google.colab import drive    # in ordre to mount drive to colab\n","drive.mount('/content/gdrive')\n","\n","gpu_options = tf.GPUOptions(allow_growth=True)\n","session_config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options, allow_soft_placement=True)\n","sess = tf.Session(config=session_config)\n","\n","batch_size = 32\n","learning_rate = 1.0e-4\n","width = 256\n","patch_width = 64\n","lambda_sl = 0.5\n","use_gan = True # change to False if not using GAN (LS_GAN).\n","if use_gan:\n","  disc_iters = 4\n","  lambda_al = 0.0025\n","else:\n","  disc_iters = 0\n","  lambda_al = 0\n","\n","num_epoch = 10"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GYR_-E3YXx9t","colab_type":"code","colab":{}},"source":["def read_data(file_name):\n","    f = h5py.File(file_name, 'r')         # read dataset\n","    label = np.array(f['data'])           \n","    f.close()\n","    return label                 # return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCt0YwfBv24Q","colab_type":"code","colab":{}},"source":["def two_dim_conv(batch, name, n_filter):\n","    c = tf.layers.conv2d(\n","        inputs=batch,\n","        filters=n_filter,\n","        kernel_size=[3, 3],\n","        strides=1,\n","        padding='same',\n","        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","        use_bias=True,\n","        activation=None,\n","        name=name,\n","        reuse=tf.AUTO_REUSE\n","    )\n","    #print(\"{}:{}\".format(name, c.get_shape()))\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L58jHLDFt97L","colab_type":"code","colab":{}},"source":["def U_res_block_layer(batch, name, n_out):\n","    conv_1 = two_dim_conv(batch, name + 'res_conv1', n_out)\n","    conv_1 = tf.nn.relu(conv_1)\n","\n","    conv_2 = two_dim_conv(conv_1, name + 'res_conv2', n_out)\n","    conv_2 = tf.nn.relu(conv_2)\n","\n","    conv_3 = two_dim_conv(conv_2, name + 'res_conv3', n_out)\n","    block_res = tf.nn.relu(conv_3) + batch\n","\n","    return block_res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7meU2WOUWekI","colab_type":"code","colab":{}},"source":["def generator(x, padding='Valid'):\n","    outputs1 = tf.layers.conv2d(x, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1', use_bias=True)\n","    outputs2 = tf.nn.relu(outputs1)\n","    # 2-D convolutional layers\n","    outputs2 = U_res_block_layer(outputs2, \"res1\", 32)\n","    \n","\n","    outputs2 = tf.layers.conv2d(outputs2, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2', use_bias=False)\n","    outputs3 = tf.nn.relu(outputs2)\n","    outputs3 = U_res_block_layer(outputs3, \"res2\", 32)\n","    \n","\n","    outputs3 = tf.layers.conv2d(outputs3, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv3', use_bias=False)\n","    outputs4 = tf.nn.relu(outputs3)\n","    outputs4 = U_res_block_layer(outputs4, \"res3\", 32)\n","\n","    outputs4 = tf.layers.conv2d(outputs4, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv4', use_bias=False)\n","    outputs5 = tf.nn.relu(outputs4)\n","    outputs5 = U_res_block_layer(outputs5, \"res4\", 32)\n","    \n","    outputs5 = tf.layers.conv2d_transpose(outputs5, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv5', \n","                                          use_bias=False)\n","    # transpose 2-D convolutional layers\n","    outputs5 = tf.nn.relu(outputs5)\n","    outputs5 = U_res_block_layer(outputs5, \"res5\", 32)\n","    outputs5 = tf.concat([outputs3, outputs5], -1)\n","\n","    outputs6 = tf.layers.conv2d_transpose(outputs5, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv6', use_bias=False)\n","    outputs6 = tf.nn.relu(outputs6)    \n","    outputs6 = U_res_block_layer(outputs6, \"res6\", 32)\n","    outputs6 = tf.concat([outputs2, outputs6], -1)\n","\n","    outputs7 = tf.layers.conv2d_transpose(outputs6, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv7', use_bias=False)\n","    outputs7 = tf.nn.relu(outputs7)\n","    outputs7 = U_res_block_layer(outputs7, \"res7\", 32)\n","    outputs7 = tf.concat([outputs1, outputs7], -1)\n","\n","    outputs8 = tf.layers.conv2d_transpose(outputs7, 1, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv8', use_bias=False)\n","    outputs = tf.nn.relu(outputs8)\n","    outputs = U_res_block_layer(outputs, \"res8\", 1)\n","    \n","    return outputs \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1N-ujiOfzK-","colab_type":"code","colab":{}},"source":["def discriminator(x):  \n","    outputs = tf.layers.conv2d(x, 64, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1')\n","    outputs = tf.nn.relu(outputs)\n","        \n","    outputs = tf.layers.conv2d(outputs, 64, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv3')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv4')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv5')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv6')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.dense(outputs, units=1024, name='dense1')\n","    outputs = tf.nn.relu(outputs)\n","    outputs = tf.layers.dense(outputs, units=1, name='dense2')\n","    return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqHrZIWzWy93","colab_type":"code","outputId":"df877c14-1ccb-47e5-bbb3-91420b45abfe","executionInfo":{"status":"ok","timestamp":1572289836395,"user_tz":240,"elapsed":341999,"user":{"displayName":"xd xd","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAgTHZWotfUszvYUVAYAD0KBsJyLtefel7POB-l=s64","userId":"07693667503497446418"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["lr = tf.placeholder(dtype=tf.float32, shape=[])  # learning rate\n","real_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, patch_width, patch_width, 1], name='real')        #real images\n","fake_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, patch_width, patch_width, 1], name='fake')        #fake images\n","fake_placeholder_test = tf.placeholder(dtype=tf.float32, shape=[1, width, width, 1], name='fake_test')                   #fakeing tesing images\n","with tf.variable_scope('generator_model', reuse=tf.AUTO_REUSE) as scope_generator_model:\n","    Gz = generator(fake_placeholder)                 # feed patches for training\n","    Gz_test = generator(fake_placeholder_test)       # feed whole images for testing\n","\n","with tf.variable_scope('discriminator_model', reuse=tf.AUTO_REUSE) as scope_discriminator_model:\n","    FD_d = discriminator(real_placeholder)           # feed real images into discriminator network\n","    LD_d = discriminator(Gz)                         # feed fake images into discriminator network\n","\n","\n","disc_loss = tf.reduce_mean((FD_d - 1)**2 + (LD_d - 0)**2)       # loss function for LS-GAN\n","disc_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator_model')\n","d_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(disc_loss, var_list=disc_variables) # training discriminator\n","\n","ssim = tf.squeeze(tf.reduce_mean(tf.image.ssim(Gz, real_placeholder, 1.0)))\n","ssim_loss = 1 - ssim\n","mse = tf.reduce_mean(tf.squared_difference(Gz, real_placeholder))\n","gen_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_model')\n","gen_cost = tf.reduce_mean((LD_d - 1)**2)          # GAN loss for generator\n","\n","g_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(mse + lambda_al * gen_cost + lambda_sl * ssim_loss, var_list=gen_variables) # training generator\n","\n","saver = tf.train.Saver()\n","print(\"enter Session\")\n","sess.run(tf.global_variables_initializer())       # initialize parameters\n","#saver.restore(sess, '/content/gdrive/My Drive/hands_on4/Autoencoder/SIGmathmodel100.ckpt')  # use to restore model\n","print('model restored')\n","print(\"Begin training\")\n","label = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Train/Label.hdf5') # read label\n","data = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Train/Data.hdf5') # read data\n","\n","\n","for iteration in range(num_epoch):\n","    #break\n","    label, data = shuffle(label, data)  # shuffling data & label, same as last time\n","    val_lr = learning_rate / (iteration + 1)       # learning rate is decreased\n","    num_batches = data.shape[0] // batch_size      # how many batches?\n","    for i in range(num_batches):\n","        for _ in range(disc_iters): # disc_iters will be set as 0 if you de-activate GAN\n","            idx = np.random.permutation(label.shape[0])       # the input to the discriminator and the input to the generator should be different\n","            batch_label = label[idx[:batch_size]]\n","            batch_label = np.expand_dims(batch_label, -1)\n","            \n","            batch_data = data[idx[:batch_size]]\n","            batch_data = np.expand_dims(batch_data, -1)\n","            \n","            _ = sess.run(d_trainer, feed_dict={real_placeholder: batch_label, # training discriminator\n","                                               fake_placeholder: batch_data,\n","                                               lr: val_lr}) \n","            \n","        batch_label = label[i * batch_size: (i + 1) * batch_size] # the input to the discriminator and the input to the generator should be different\n","        batch_label = np.expand_dims(batch_label, -1)\n","        batch_data = data[i * batch_size: (i + 1) * batch_size]\n","        batch_data = np.expand_dims(batch_data, -1)\n","        _mse, _ssim, _ = sess.run([mse, ssim, g_trainer], feed_dict={real_placeholder: (batch_label),  # training generator\n","                                                                             fake_placeholder: (batch_data), lr: val_lr})\n","\n","        print('Epoch: %d - %d - mse %.6f: - ssim %.6f: ' % (iteration, i, _mse, _ssim))\n","\n","saver.save(sess, '/content/gdrive/My Drive/hands_on4/Autoencoder/SIGmathmodel100.ckpt')  # save model\n","  \n","print(\"testing dataset\")\n","test_label = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Test/Label.hdf5')   # reading testing label\n","test_data = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Test/Data.hdf5')   # reaing testing data\n","for c in range(test_label.shape[0]):\n","    test_batch_label = test_label[c * 1: (c + 1) * 1]  # feed images one by one\n","    test_batch_label = np.expand_dims(test_batch_label, -1)\n","    \n","    test_batch_data = test_data[c * 1: (c + 1) * 1]\n","    test_batch_data = np.expand_dims(test_batch_data, -1)\n","    \n","    with tf.variable_scope('generator_model') as scope:\n","        scope.reuse_variables()\n","        estimated = sess.run(Gz_test, feed_dict={fake_placeholder_test: test_batch_data})      # testing\n","        t_r = np.squeeze(test_batch_label)\n","        e_1 = np.squeeze(estimated)\n","        t_o = np.squeeze(test_batch_data)\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'validation.jpg' #save images\n","        plt.imsave(f_n, e_1, cmap='gray')\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'real_.jpg'\n","        plt.imsave(f_n, t_r, cmap='gray')\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'original_.jpg'\n","        plt.imsave(f_n, t_o, cmap='gray')\n","\n","        z = (t_r - e_1) ** 2\n","        print(np.mean(z))   # print mse loss."],"execution_count":32,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-31-f211d37a5e13>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","enter Session\n","model restored\n","Begin training\n","Epoch: 0 - 0 - mse 0.011783: - ssim 0.806754: \n","Epoch: 0 - 1 - mse 0.013204: - ssim 0.777343: \n","Epoch: 0 - 2 - mse 0.007774: - ssim 0.840091: \n","Epoch: 0 - 3 - mse 0.006061: - ssim 0.865351: \n","Epoch: 0 - 4 - mse 0.005497: - ssim 0.884312: \n","Epoch: 0 - 5 - mse 0.003991: - ssim 0.901705: \n","Epoch: 0 - 6 - mse 0.004053: - ssim 0.903627: \n","Epoch: 0 - 7 - mse 0.005517: - ssim 0.891854: \n","Epoch: 0 - 8 - mse 0.003448: - ssim 0.923909: \n","Epoch: 0 - 9 - mse 0.003692: - ssim 0.921499: \n","Epoch: 0 - 10 - mse 0.003973: - ssim 0.903666: \n","Epoch: 0 - 11 - mse 0.003400: - ssim 0.929904: \n","Epoch: 0 - 12 - mse 0.003254: - ssim 0.926852: \n","Epoch: 0 - 13 - mse 0.002584: - ssim 0.928867: \n","Epoch: 0 - 14 - mse 0.003384: - ssim 0.929584: \n","Epoch: 0 - 15 - mse 0.003493: - ssim 0.928930: \n","Epoch: 0 - 16 - mse 0.003459: - ssim 0.932453: \n","Epoch: 0 - 17 - mse 0.003413: - ssim 0.937551: \n","Epoch: 0 - 18 - mse 0.003660: - ssim 0.936570: \n","Epoch: 0 - 19 - mse 0.003588: - ssim 0.930908: \n","Epoch: 0 - 20 - mse 0.004267: - ssim 0.922325: \n","Epoch: 0 - 21 - mse 0.003597: - ssim 0.934392: \n","Epoch: 0 - 22 - mse 0.003304: - ssim 0.935439: \n","Epoch: 0 - 23 - mse 0.003907: - ssim 0.932061: \n","Epoch: 0 - 24 - mse 0.003124: - ssim 0.940816: \n","Epoch: 0 - 25 - mse 0.003048: - ssim 0.939093: \n","Epoch: 0 - 26 - mse 0.003505: - ssim 0.947017: \n","Epoch: 0 - 27 - mse 0.003443: - ssim 0.937717: \n","Epoch: 0 - 28 - mse 0.002120: - ssim 0.951092: \n","Epoch: 0 - 29 - mse 0.003032: - ssim 0.946667: \n","Epoch: 0 - 30 - mse 0.003616: - ssim 0.941034: \n","Epoch: 0 - 31 - mse 0.003555: - ssim 0.939059: \n","Epoch: 0 - 32 - mse 0.002452: - ssim 0.955683: \n","Epoch: 0 - 33 - mse 0.003106: - ssim 0.941697: \n","Epoch: 0 - 34 - mse 0.003193: - ssim 0.945967: \n","Epoch: 0 - 35 - mse 0.003736: - ssim 0.937659: \n","Epoch: 0 - 36 - mse 0.003716: - ssim 0.942027: \n","Epoch: 0 - 37 - mse 0.002976: - ssim 0.946546: \n","Epoch: 0 - 38 - mse 0.002810: - ssim 0.948428: \n","Epoch: 0 - 39 - mse 0.003083: - ssim 0.949556: \n","Epoch: 0 - 40 - mse 0.002766: - ssim 0.952634: \n","Epoch: 0 - 41 - mse 0.002163: - ssim 0.954171: \n","Epoch: 0 - 42 - mse 0.002782: - ssim 0.949086: \n","Epoch: 0 - 43 - mse 0.002900: - ssim 0.951299: \n","Epoch: 0 - 44 - mse 0.003577: - ssim 0.939433: \n","Epoch: 0 - 45 - mse 0.002951: - ssim 0.943867: \n","Epoch: 0 - 46 - mse 0.002943: - ssim 0.950267: \n","Epoch: 0 - 47 - mse 0.002843: - ssim 0.948863: \n","Epoch: 0 - 48 - mse 0.002684: - ssim 0.948910: \n","Epoch: 0 - 49 - mse 0.003395: - ssim 0.941918: \n","Epoch: 1 - 0 - mse 0.002769: - ssim 0.949540: \n","Epoch: 1 - 1 - mse 0.002689: - ssim 0.946495: \n","Epoch: 1 - 2 - mse 0.001738: - ssim 0.965191: \n","Epoch: 1 - 3 - mse 0.003005: - ssim 0.949261: \n","Epoch: 1 - 4 - mse 0.002900: - ssim 0.947263: \n","Epoch: 1 - 5 - mse 0.003205: - ssim 0.943456: \n","Epoch: 1 - 6 - mse 0.002738: - ssim 0.947005: \n","Epoch: 1 - 7 - mse 0.002456: - ssim 0.958510: \n","Epoch: 1 - 8 - mse 0.003332: - ssim 0.945046: \n","Epoch: 1 - 9 - mse 0.002251: - ssim 0.957523: \n","Epoch: 1 - 10 - mse 0.002494: - ssim 0.961832: \n","Epoch: 1 - 11 - mse 0.002783: - ssim 0.952959: \n","Epoch: 1 - 12 - mse 0.003569: - ssim 0.939489: \n","Epoch: 1 - 13 - mse 0.004153: - ssim 0.936452: \n","Epoch: 1 - 14 - mse 0.004839: - ssim 0.922861: \n","Epoch: 1 - 15 - mse 0.002555: - ssim 0.950756: \n","Epoch: 1 - 16 - mse 0.002452: - ssim 0.955270: \n","Epoch: 1 - 17 - mse 0.002316: - ssim 0.960477: \n","Epoch: 1 - 18 - mse 0.002104: - ssim 0.966329: \n","Epoch: 1 - 19 - mse 0.002577: - ssim 0.956058: \n","Epoch: 1 - 20 - mse 0.002916: - ssim 0.948581: \n","Epoch: 1 - 21 - mse 0.004735: - ssim 0.927816: \n","Epoch: 1 - 22 - mse 0.002271: - ssim 0.959450: \n","Epoch: 1 - 23 - mse 0.003394: - ssim 0.943436: \n","Epoch: 1 - 24 - mse 0.002455: - ssim 0.947945: \n","Epoch: 1 - 25 - mse 0.002400: - ssim 0.953655: \n","Epoch: 1 - 26 - mse 0.002546: - ssim 0.949792: \n","Epoch: 1 - 27 - mse 0.002291: - ssim 0.958437: \n","Epoch: 1 - 28 - mse 0.002878: - ssim 0.953227: \n","Epoch: 1 - 29 - mse 0.002377: - ssim 0.951658: \n","Epoch: 1 - 30 - mse 0.002507: - ssim 0.954665: \n","Epoch: 1 - 31 - mse 0.002400: - ssim 0.957333: \n","Epoch: 1 - 32 - mse 0.003319: - ssim 0.941407: \n","Epoch: 1 - 33 - mse 0.003198: - ssim 0.950087: \n","Epoch: 1 - 34 - mse 0.002815: - ssim 0.956149: \n","Epoch: 1 - 35 - mse 0.003138: - ssim 0.942708: \n","Epoch: 1 - 36 - mse 0.002804: - ssim 0.945542: \n","Epoch: 1 - 37 - mse 0.003398: - ssim 0.941636: \n","Epoch: 1 - 38 - mse 0.002270: - ssim 0.959636: \n","Epoch: 1 - 39 - mse 0.002723: - ssim 0.945486: \n","Epoch: 1 - 40 - mse 0.002888: - ssim 0.950022: \n","Epoch: 1 - 41 - mse 0.002510: - ssim 0.954497: \n","Epoch: 1 - 42 - mse 0.002759: - ssim 0.948285: \n","Epoch: 1 - 43 - mse 0.002845: - ssim 0.948008: \n","Epoch: 1 - 44 - mse 0.002206: - ssim 0.961273: \n","Epoch: 1 - 45 - mse 0.002935: - ssim 0.944894: \n","Epoch: 1 - 46 - mse 0.003434: - ssim 0.944144: \n","Epoch: 1 - 47 - mse 0.003087: - ssim 0.945620: \n","Epoch: 1 - 48 - mse 0.002514: - ssim 0.953796: \n","Epoch: 1 - 49 - mse 0.002504: - ssim 0.952266: \n","Epoch: 2 - 0 - mse 0.003103: - ssim 0.948493: \n","Epoch: 2 - 1 - mse 0.002461: - ssim 0.954985: \n","Epoch: 2 - 2 - mse 0.002501: - ssim 0.953020: \n","Epoch: 2 - 3 - mse 0.002198: - ssim 0.957134: \n","Epoch: 2 - 4 - mse 0.002091: - ssim 0.960776: \n","Epoch: 2 - 5 - mse 0.002810: - ssim 0.957713: \n","Epoch: 2 - 6 - mse 0.002160: - ssim 0.954756: \n","Epoch: 2 - 7 - mse 0.003106: - ssim 0.946667: \n","Epoch: 2 - 8 - mse 0.002789: - ssim 0.951901: \n","Epoch: 2 - 9 - mse 0.002220: - ssim 0.957295: \n","Epoch: 2 - 10 - mse 0.002288: - ssim 0.961971: \n","Epoch: 2 - 11 - mse 0.003085: - ssim 0.948247: \n","Epoch: 2 - 12 - mse 0.002152: - ssim 0.958249: \n","Epoch: 2 - 13 - mse 0.002416: - ssim 0.952750: \n","Epoch: 2 - 14 - mse 0.003013: - ssim 0.946702: \n","Epoch: 2 - 15 - mse 0.002392: - ssim 0.958101: \n","Epoch: 2 - 16 - mse 0.003198: - ssim 0.948234: \n","Epoch: 2 - 17 - mse 0.002726: - ssim 0.949835: \n","Epoch: 2 - 18 - mse 0.002386: - ssim 0.960566: \n","Epoch: 2 - 19 - mse 0.003877: - ssim 0.945045: \n","Epoch: 2 - 20 - mse 0.002841: - ssim 0.945026: \n","Epoch: 2 - 21 - mse 0.002895: - ssim 0.946036: \n","Epoch: 2 - 22 - mse 0.002659: - ssim 0.951515: \n","Epoch: 2 - 23 - mse 0.002059: - ssim 0.963247: \n","Epoch: 2 - 24 - mse 0.002262: - ssim 0.953661: \n","Epoch: 2 - 25 - mse 0.002706: - ssim 0.952175: \n","Epoch: 2 - 26 - mse 0.002042: - ssim 0.963403: \n","Epoch: 2 - 27 - mse 0.001864: - ssim 0.962413: \n","Epoch: 2 - 28 - mse 0.002842: - ssim 0.951833: \n","Epoch: 2 - 29 - mse 0.002431: - ssim 0.952827: \n","Epoch: 2 - 30 - mse 0.002537: - ssim 0.953275: \n","Epoch: 2 - 31 - mse 0.002882: - ssim 0.946421: \n","Epoch: 2 - 32 - mse 0.002743: - ssim 0.952590: \n","Epoch: 2 - 33 - mse 0.002801: - ssim 0.950044: \n","Epoch: 2 - 34 - mse 0.003206: - ssim 0.943956: \n","Epoch: 2 - 35 - mse 0.002755: - ssim 0.943423: \n","Epoch: 2 - 36 - mse 0.002800: - ssim 0.951816: \n","Epoch: 2 - 37 - mse 0.002710: - ssim 0.949959: \n","Epoch: 2 - 38 - mse 0.002992: - ssim 0.945440: \n","Epoch: 2 - 39 - mse 0.002584: - ssim 0.953116: \n","Epoch: 2 - 40 - mse 0.002904: - ssim 0.955477: \n","Epoch: 2 - 41 - mse 0.001536: - ssim 0.960005: \n","Epoch: 2 - 42 - mse 0.003372: - ssim 0.937192: \n","Epoch: 2 - 43 - mse 0.002024: - ssim 0.957324: \n","Epoch: 2 - 44 - mse 0.003352: - ssim 0.941375: \n","Epoch: 2 - 45 - mse 0.002480: - ssim 0.955745: \n","Epoch: 2 - 46 - mse 0.002751: - ssim 0.952897: \n","Epoch: 2 - 47 - mse 0.002830: - ssim 0.948237: \n","Epoch: 2 - 48 - mse 0.002245: - ssim 0.958582: \n","Epoch: 2 - 49 - mse 0.002298: - ssim 0.959536: \n","Epoch: 3 - 0 - mse 0.002710: - ssim 0.947869: \n","Epoch: 3 - 1 - mse 0.002690: - ssim 0.952533: \n","Epoch: 3 - 2 - mse 0.002797: - ssim 0.953053: \n","Epoch: 3 - 3 - mse 0.002367: - ssim 0.956659: \n","Epoch: 3 - 4 - mse 0.002245: - ssim 0.956695: \n","Epoch: 3 - 5 - mse 0.002481: - ssim 0.955402: \n","Epoch: 3 - 6 - mse 0.002933: - ssim 0.950237: \n","Epoch: 3 - 7 - mse 0.002987: - ssim 0.948671: \n","Epoch: 3 - 8 - mse 0.001724: - ssim 0.963929: \n","Epoch: 3 - 9 - mse 0.002996: - ssim 0.946148: \n","Epoch: 3 - 10 - mse 0.002108: - ssim 0.961803: \n","Epoch: 3 - 11 - mse 0.003407: - ssim 0.946082: \n","Epoch: 3 - 12 - mse 0.002848: - ssim 0.949264: \n","Epoch: 3 - 13 - mse 0.002630: - ssim 0.948178: \n","Epoch: 3 - 14 - mse 0.003465: - ssim 0.947274: \n","Epoch: 3 - 15 - mse 0.002865: - ssim 0.947455: \n","Epoch: 3 - 16 - mse 0.002638: - ssim 0.951797: \n","Epoch: 3 - 17 - mse 0.002350: - ssim 0.958608: \n","Epoch: 3 - 18 - mse 0.001871: - ssim 0.960766: \n","Epoch: 3 - 19 - mse 0.002683: - ssim 0.950976: \n","Epoch: 3 - 20 - mse 0.003311: - ssim 0.943751: \n","Epoch: 3 - 21 - mse 0.002872: - ssim 0.951990: \n","Epoch: 3 - 22 - mse 0.002997: - ssim 0.947959: \n","Epoch: 3 - 23 - mse 0.001967: - ssim 0.964687: \n","Epoch: 3 - 24 - mse 0.001980: - ssim 0.966267: \n","Epoch: 3 - 25 - mse 0.002193: - ssim 0.955045: \n","Epoch: 3 - 26 - mse 0.002838: - ssim 0.955941: \n","Epoch: 3 - 27 - mse 0.002783: - ssim 0.950481: \n","Epoch: 3 - 28 - mse 0.002025: - ssim 0.960574: \n","Epoch: 3 - 29 - mse 0.002019: - ssim 0.962706: \n","Epoch: 3 - 30 - mse 0.002340: - ssim 0.951459: \n","Epoch: 3 - 31 - mse 0.001843: - ssim 0.959707: \n","Epoch: 3 - 32 - mse 0.002676: - ssim 0.952690: \n","Epoch: 3 - 33 - mse 0.002972: - ssim 0.945785: \n","Epoch: 3 - 34 - mse 0.001800: - ssim 0.955210: \n","Epoch: 3 - 35 - mse 0.002312: - ssim 0.957132: \n","Epoch: 3 - 36 - mse 0.001524: - ssim 0.965449: \n","Epoch: 3 - 37 - mse 0.001955: - ssim 0.956063: \n","Epoch: 3 - 38 - mse 0.002061: - ssim 0.955425: \n","Epoch: 3 - 39 - mse 0.002916: - ssim 0.947421: \n","Epoch: 3 - 40 - mse 0.002069: - ssim 0.961036: \n","Epoch: 3 - 41 - mse 0.002604: - ssim 0.948029: \n","Epoch: 3 - 42 - mse 0.002868: - ssim 0.951967: \n","Epoch: 3 - 43 - mse 0.003019: - ssim 0.949224: \n","Epoch: 3 - 44 - mse 0.002391: - ssim 0.955837: \n","Epoch: 3 - 45 - mse 0.001968: - ssim 0.957935: \n","Epoch: 3 - 46 - mse 0.002010: - ssim 0.962623: \n","Epoch: 3 - 47 - mse 0.002105: - ssim 0.957091: \n","Epoch: 3 - 48 - mse 0.002590: - ssim 0.950750: \n","Epoch: 3 - 49 - mse 0.002341: - ssim 0.953972: \n","Epoch: 4 - 0 - mse 0.002604: - ssim 0.948941: \n","Epoch: 4 - 1 - mse 0.002160: - ssim 0.955469: \n","Epoch: 4 - 2 - mse 0.002572: - ssim 0.953311: \n","Epoch: 4 - 3 - mse 0.001997: - ssim 0.962751: \n","Epoch: 4 - 4 - mse 0.002142: - ssim 0.960197: \n","Epoch: 4 - 5 - mse 0.002617: - ssim 0.952046: \n","Epoch: 4 - 6 - mse 0.001603: - ssim 0.964259: \n","Epoch: 4 - 7 - mse 0.002806: - ssim 0.951883: \n","Epoch: 4 - 8 - mse 0.001411: - ssim 0.969600: \n","Epoch: 4 - 9 - mse 0.002340: - ssim 0.954513: \n","Epoch: 4 - 10 - mse 0.002612: - ssim 0.950182: \n","Epoch: 4 - 11 - mse 0.002263: - ssim 0.958246: \n","Epoch: 4 - 12 - mse 0.002753: - ssim 0.950061: \n","Epoch: 4 - 13 - mse 0.001775: - ssim 0.961335: \n","Epoch: 4 - 14 - mse 0.002422: - ssim 0.953566: \n","Epoch: 4 - 15 - mse 0.002720: - ssim 0.953541: \n","Epoch: 4 - 16 - mse 0.002119: - ssim 0.962465: \n","Epoch: 4 - 17 - mse 0.002860: - ssim 0.946608: \n","Epoch: 4 - 18 - mse 0.001889: - ssim 0.965322: \n","Epoch: 4 - 19 - mse 0.002118: - ssim 0.955286: \n","Epoch: 4 - 20 - mse 0.002073: - ssim 0.958822: \n","Epoch: 4 - 21 - mse 0.003221: - ssim 0.945014: \n","Epoch: 4 - 22 - mse 0.003516: - ssim 0.941030: \n","Epoch: 4 - 23 - mse 0.002331: - ssim 0.958064: \n","Epoch: 4 - 24 - mse 0.002222: - ssim 0.961333: \n","Epoch: 4 - 25 - mse 0.002669: - ssim 0.949030: \n","Epoch: 4 - 26 - mse 0.002615: - ssim 0.951245: \n","Epoch: 4 - 27 - mse 0.001969: - ssim 0.958201: \n","Epoch: 4 - 28 - mse 0.002080: - ssim 0.955821: \n","Epoch: 4 - 29 - mse 0.003569: - ssim 0.937702: \n","Epoch: 4 - 30 - mse 0.002351: - ssim 0.955151: \n","Epoch: 4 - 31 - mse 0.002572: - ssim 0.953014: \n","Epoch: 4 - 32 - mse 0.003066: - ssim 0.946310: \n","Epoch: 4 - 33 - mse 0.002623: - ssim 0.957782: \n","Epoch: 4 - 34 - mse 0.002038: - ssim 0.954409: \n","Epoch: 4 - 35 - mse 0.002125: - ssim 0.961166: \n","Epoch: 4 - 36 - mse 0.001897: - ssim 0.960470: \n","Epoch: 4 - 37 - mse 0.001715: - ssim 0.963194: \n","Epoch: 4 - 38 - mse 0.002399: - ssim 0.950999: \n","Epoch: 4 - 39 - mse 0.002677: - ssim 0.953929: \n","Epoch: 4 - 40 - mse 0.002349: - ssim 0.957219: \n","Epoch: 4 - 41 - mse 0.002298: - ssim 0.953763: \n","Epoch: 4 - 42 - mse 0.003316: - ssim 0.940939: \n","Epoch: 4 - 43 - mse 0.001915: - ssim 0.963405: \n","Epoch: 4 - 44 - mse 0.001876: - ssim 0.961351: \n","Epoch: 4 - 45 - mse 0.002209: - ssim 0.956526: \n","Epoch: 4 - 46 - mse 0.002445: - ssim 0.955646: \n","Epoch: 4 - 47 - mse 0.002301: - ssim 0.958363: \n","Epoch: 4 - 48 - mse 0.002110: - ssim 0.953596: \n","Epoch: 4 - 49 - mse 0.002180: - ssim 0.960689: \n","Epoch: 5 - 0 - mse 0.002129: - ssim 0.958476: \n","Epoch: 5 - 1 - mse 0.002666: - ssim 0.952396: \n","Epoch: 5 - 2 - mse 0.002294: - ssim 0.957772: \n","Epoch: 5 - 3 - mse 0.001867: - ssim 0.961602: \n","Epoch: 5 - 4 - mse 0.002520: - ssim 0.953595: \n","Epoch: 5 - 5 - mse 0.002031: - ssim 0.960700: \n","Epoch: 5 - 6 - mse 0.001859: - ssim 0.966406: \n","Epoch: 5 - 7 - mse 0.001760: - ssim 0.962151: \n","Epoch: 5 - 8 - mse 0.002427: - ssim 0.949649: \n","Epoch: 5 - 9 - mse 0.001669: - ssim 0.970967: \n","Epoch: 5 - 10 - mse 0.003119: - ssim 0.945683: \n","Epoch: 5 - 11 - mse 0.001984: - ssim 0.962623: \n","Epoch: 5 - 12 - mse 0.002263: - ssim 0.955452: \n","Epoch: 5 - 13 - mse 0.003150: - ssim 0.944743: \n","Epoch: 5 - 14 - mse 0.002062: - ssim 0.954300: \n","Epoch: 5 - 15 - mse 0.002539: - ssim 0.951214: \n","Epoch: 5 - 16 - mse 0.002749: - ssim 0.948410: \n","Epoch: 5 - 17 - mse 0.003283: - ssim 0.944210: \n","Epoch: 5 - 18 - mse 0.003072: - ssim 0.944439: \n","Epoch: 5 - 19 - mse 0.002192: - ssim 0.961907: \n","Epoch: 5 - 20 - mse 0.002281: - ssim 0.956150: \n","Epoch: 5 - 21 - mse 0.002452: - ssim 0.953163: \n","Epoch: 5 - 22 - mse 0.001985: - ssim 0.961623: \n","Epoch: 5 - 23 - mse 0.002015: - ssim 0.954488: \n","Epoch: 5 - 24 - mse 0.001930: - ssim 0.958115: \n","Epoch: 5 - 25 - mse 0.001422: - ssim 0.967973: \n","Epoch: 5 - 26 - mse 0.002445: - ssim 0.956833: \n","Epoch: 5 - 27 - mse 0.001679: - ssim 0.967597: \n","Epoch: 5 - 28 - mse 0.002618: - ssim 0.949147: \n","Epoch: 5 - 29 - mse 0.002530: - ssim 0.951763: \n","Epoch: 5 - 30 - mse 0.002155: - ssim 0.955219: \n","Epoch: 5 - 31 - mse 0.002554: - ssim 0.958614: \n","Epoch: 5 - 32 - mse 0.002674: - ssim 0.952236: \n","Epoch: 5 - 33 - mse 0.001919: - ssim 0.961474: \n","Epoch: 5 - 34 - mse 0.002286: - ssim 0.953082: \n","Epoch: 5 - 35 - mse 0.003019: - ssim 0.940408: \n","Epoch: 5 - 36 - mse 0.001363: - ssim 0.970533: \n","Epoch: 5 - 37 - mse 0.002256: - ssim 0.952509: \n","Epoch: 5 - 38 - mse 0.002051: - ssim 0.961811: \n","Epoch: 5 - 39 - mse 0.001780: - ssim 0.963652: \n","Epoch: 5 - 40 - mse 0.002151: - ssim 0.957526: \n","Epoch: 5 - 41 - mse 0.001997: - ssim 0.962282: \n","Epoch: 5 - 42 - mse 0.002607: - ssim 0.949419: \n","Epoch: 5 - 43 - mse 0.003054: - ssim 0.949429: \n","Epoch: 5 - 44 - mse 0.002316: - ssim 0.956403: \n","Epoch: 5 - 45 - mse 0.002487: - ssim 0.954247: \n","Epoch: 5 - 46 - mse 0.001974: - ssim 0.959181: \n","Epoch: 5 - 47 - mse 0.002249: - ssim 0.954380: \n","Epoch: 5 - 48 - mse 0.001634: - ssim 0.956472: \n","Epoch: 5 - 49 - mse 0.002408: - ssim 0.956091: \n","Epoch: 6 - 0 - mse 0.001959: - ssim 0.962716: \n","Epoch: 6 - 1 - mse 0.001669: - ssim 0.962417: \n","Epoch: 6 - 2 - mse 0.003107: - ssim 0.948999: \n","Epoch: 6 - 3 - mse 0.002529: - ssim 0.953364: \n","Epoch: 6 - 4 - mse 0.002083: - ssim 0.955560: \n","Epoch: 6 - 5 - mse 0.002617: - ssim 0.951073: \n","Epoch: 6 - 6 - mse 0.002020: - ssim 0.960777: \n","Epoch: 6 - 7 - mse 0.002288: - ssim 0.954782: \n","Epoch: 6 - 8 - mse 0.002200: - ssim 0.954840: \n","Epoch: 6 - 9 - mse 0.002785: - ssim 0.952198: \n","Epoch: 6 - 10 - mse 0.002142: - ssim 0.960815: \n","Epoch: 6 - 11 - mse 0.001760: - ssim 0.969384: \n","Epoch: 6 - 12 - mse 0.001653: - ssim 0.963684: \n","Epoch: 6 - 13 - mse 0.001772: - ssim 0.964637: \n","Epoch: 6 - 14 - mse 0.002568: - ssim 0.956135: \n","Epoch: 6 - 15 - mse 0.001752: - ssim 0.959733: \n","Epoch: 6 - 16 - mse 0.001932: - ssim 0.956640: \n","Epoch: 6 - 17 - mse 0.002928: - ssim 0.949060: \n","Epoch: 6 - 18 - mse 0.002119: - ssim 0.952196: \n","Epoch: 6 - 19 - mse 0.002557: - ssim 0.947859: \n","Epoch: 6 - 20 - mse 0.002080: - ssim 0.956639: \n","Epoch: 6 - 21 - mse 0.002184: - ssim 0.956611: \n","Epoch: 6 - 22 - mse 0.002581: - ssim 0.951962: \n","Epoch: 6 - 23 - mse 0.001759: - ssim 0.966474: \n","Epoch: 6 - 24 - mse 0.001810: - ssim 0.957744: \n","Epoch: 6 - 25 - mse 0.002219: - ssim 0.957714: \n","Epoch: 6 - 26 - mse 0.002715: - ssim 0.952575: \n","Epoch: 6 - 27 - mse 0.002394: - ssim 0.959882: \n","Epoch: 6 - 28 - mse 0.003136: - ssim 0.946798: \n","Epoch: 6 - 29 - mse 0.001985: - ssim 0.956665: \n","Epoch: 6 - 30 - mse 0.001939: - ssim 0.962440: \n","Epoch: 6 - 31 - mse 0.002446: - ssim 0.957790: \n","Epoch: 6 - 32 - mse 0.001995: - ssim 0.962903: \n","Epoch: 6 - 33 - mse 0.001788: - ssim 0.961132: \n","Epoch: 6 - 34 - mse 0.002007: - ssim 0.960088: \n","Epoch: 6 - 35 - mse 0.002940: - ssim 0.943635: \n","Epoch: 6 - 36 - mse 0.001798: - ssim 0.962542: \n","Epoch: 6 - 37 - mse 0.002961: - ssim 0.943055: \n","Epoch: 6 - 38 - mse 0.002209: - ssim 0.955878: \n","Epoch: 6 - 39 - mse 0.001681: - ssim 0.964108: \n","Epoch: 6 - 40 - mse 0.001893: - ssim 0.958848: \n","Epoch: 6 - 41 - mse 0.001609: - ssim 0.968085: \n","Epoch: 6 - 42 - mse 0.001590: - ssim 0.963061: \n","Epoch: 6 - 43 - mse 0.002867: - ssim 0.942025: \n","Epoch: 6 - 44 - mse 0.002094: - ssim 0.956979: \n","Epoch: 6 - 45 - mse 0.002207: - ssim 0.954584: \n","Epoch: 6 - 46 - mse 0.001550: - ssim 0.963606: \n","Epoch: 6 - 47 - mse 0.002629: - ssim 0.947564: \n","Epoch: 6 - 48 - mse 0.002433: - ssim 0.957808: \n","Epoch: 6 - 49 - mse 0.001873: - ssim 0.962570: \n","Epoch: 7 - 0 - mse 0.003001: - ssim 0.943155: \n","Epoch: 7 - 1 - mse 0.001717: - ssim 0.961478: \n","Epoch: 7 - 2 - mse 0.002595: - ssim 0.952939: \n","Epoch: 7 - 3 - mse 0.002343: - ssim 0.956152: \n","Epoch: 7 - 4 - mse 0.001713: - ssim 0.965402: \n","Epoch: 7 - 5 - mse 0.001911: - ssim 0.962657: \n","Epoch: 7 - 6 - mse 0.001641: - ssim 0.960446: \n","Epoch: 7 - 7 - mse 0.001927: - ssim 0.958533: \n","Epoch: 7 - 8 - mse 0.002125: - ssim 0.960489: \n","Epoch: 7 - 9 - mse 0.002135: - ssim 0.960510: \n","Epoch: 7 - 10 - mse 0.003021: - ssim 0.944964: \n","Epoch: 7 - 11 - mse 0.001913: - ssim 0.961237: \n","Epoch: 7 - 12 - mse 0.002376: - ssim 0.949774: \n","Epoch: 7 - 13 - mse 0.001627: - ssim 0.966310: \n","Epoch: 7 - 14 - mse 0.002231: - ssim 0.958139: \n","Epoch: 7 - 15 - mse 0.002329: - ssim 0.952495: \n","Epoch: 7 - 16 - mse 0.002358: - ssim 0.955678: \n","Epoch: 7 - 17 - mse 0.002215: - ssim 0.953507: \n","Epoch: 7 - 18 - mse 0.001740: - ssim 0.961212: \n","Epoch: 7 - 19 - mse 0.001394: - ssim 0.966241: \n","Epoch: 7 - 20 - mse 0.002371: - ssim 0.955663: \n","Epoch: 7 - 21 - mse 0.002023: - ssim 0.958314: \n","Epoch: 7 - 22 - mse 0.001873: - ssim 0.963157: \n","Epoch: 7 - 23 - mse 0.002315: - ssim 0.957239: \n","Epoch: 7 - 24 - mse 0.002458: - ssim 0.954160: \n","Epoch: 7 - 25 - mse 0.002282: - ssim 0.952336: \n","Epoch: 7 - 26 - mse 0.002051: - ssim 0.960074: \n","Epoch: 7 - 27 - mse 0.003122: - ssim 0.943684: \n","Epoch: 7 - 28 - mse 0.002746: - ssim 0.951663: \n","Epoch: 7 - 29 - mse 0.001584: - ssim 0.963963: \n","Epoch: 7 - 30 - mse 0.002822: - ssim 0.948107: \n","Epoch: 7 - 31 - mse 0.001946: - ssim 0.964865: \n","Epoch: 7 - 32 - mse 0.002164: - ssim 0.954081: \n","Epoch: 7 - 33 - mse 0.001650: - ssim 0.962506: \n","Epoch: 7 - 34 - mse 0.001789: - ssim 0.963914: \n","Epoch: 7 - 35 - mse 0.001459: - ssim 0.960959: \n","Epoch: 7 - 36 - mse 0.002469: - ssim 0.951243: \n","Epoch: 7 - 37 - mse 0.002331: - ssim 0.957106: \n","Epoch: 7 - 38 - mse 0.002337: - ssim 0.956012: \n","Epoch: 7 - 39 - mse 0.001459: - ssim 0.967745: \n","Epoch: 7 - 40 - mse 0.001924: - ssim 0.959794: \n","Epoch: 7 - 41 - mse 0.001822: - ssim 0.962354: \n","Epoch: 7 - 42 - mse 0.001768: - ssim 0.962465: \n","Epoch: 7 - 43 - mse 0.002332: - ssim 0.952224: \n","Epoch: 7 - 44 - mse 0.002137: - ssim 0.956000: \n","Epoch: 7 - 45 - mse 0.001783: - ssim 0.964101: \n","Epoch: 7 - 46 - mse 0.002201: - ssim 0.951500: \n","Epoch: 7 - 47 - mse 0.001809: - ssim 0.961893: \n","Epoch: 7 - 48 - mse 0.002399: - ssim 0.952591: \n","Epoch: 7 - 49 - mse 0.002365: - ssim 0.955833: \n","Epoch: 8 - 0 - mse 0.002121: - ssim 0.957874: \n","Epoch: 8 - 1 - mse 0.002387: - ssim 0.949578: \n","Epoch: 8 - 2 - mse 0.002035: - ssim 0.957998: \n","Epoch: 8 - 3 - mse 0.001839: - ssim 0.961872: \n","Epoch: 8 - 4 - mse 0.001824: - ssim 0.962149: \n","Epoch: 8 - 5 - mse 0.002164: - ssim 0.957650: \n","Epoch: 8 - 6 - mse 0.001956: - ssim 0.957220: \n","Epoch: 8 - 7 - mse 0.002119: - ssim 0.953657: \n","Epoch: 8 - 8 - mse 0.001549: - ssim 0.965413: \n","Epoch: 8 - 9 - mse 0.001809: - ssim 0.961104: \n","Epoch: 8 - 10 - mse 0.001686: - ssim 0.964212: \n","Epoch: 8 - 11 - mse 0.002415: - ssim 0.959227: \n","Epoch: 8 - 12 - mse 0.001739: - ssim 0.960561: \n","Epoch: 8 - 13 - mse 0.002142: - ssim 0.960569: \n","Epoch: 8 - 14 - mse 0.002152: - ssim 0.953503: \n","Epoch: 8 - 15 - mse 0.002597: - ssim 0.948943: \n","Epoch: 8 - 16 - mse 0.001645: - ssim 0.966626: \n","Epoch: 8 - 17 - mse 0.002096: - ssim 0.955318: \n","Epoch: 8 - 18 - mse 0.002350: - ssim 0.949794: \n","Epoch: 8 - 19 - mse 0.002181: - ssim 0.955021: \n","Epoch: 8 - 20 - mse 0.002067: - ssim 0.956816: \n","Epoch: 8 - 21 - mse 0.001990: - ssim 0.957904: \n","Epoch: 8 - 22 - mse 0.001634: - ssim 0.967973: \n","Epoch: 8 - 23 - mse 0.002292: - ssim 0.955875: \n","Epoch: 8 - 24 - mse 0.001883: - ssim 0.958606: \n","Epoch: 8 - 25 - mse 0.002080: - ssim 0.959723: \n","Epoch: 8 - 26 - mse 0.002169: - ssim 0.958930: \n","Epoch: 8 - 27 - mse 0.002529: - ssim 0.948594: \n","Epoch: 8 - 28 - mse 0.001946: - ssim 0.959473: \n","Epoch: 8 - 29 - mse 0.002275: - ssim 0.958850: \n","Epoch: 8 - 30 - mse 0.001733: - ssim 0.963622: \n","Epoch: 8 - 31 - mse 0.001858: - ssim 0.959694: \n","Epoch: 8 - 32 - mse 0.001861: - ssim 0.962244: \n","Epoch: 8 - 33 - mse 0.001853: - ssim 0.961669: \n","Epoch: 8 - 34 - mse 0.002203: - ssim 0.955914: \n","Epoch: 8 - 35 - mse 0.002428: - ssim 0.950059: \n","Epoch: 8 - 36 - mse 0.001718: - ssim 0.965296: \n","Epoch: 8 - 37 - mse 0.002001: - ssim 0.957762: \n","Epoch: 8 - 38 - mse 0.002137: - ssim 0.960123: \n","Epoch: 8 - 39 - mse 0.001769: - ssim 0.960241: \n","Epoch: 8 - 40 - mse 0.001800: - ssim 0.955662: \n","Epoch: 8 - 41 - mse 0.001680: - ssim 0.964069: \n","Epoch: 8 - 42 - mse 0.001900: - ssim 0.960801: \n","Epoch: 8 - 43 - mse 0.002749: - ssim 0.955056: \n","Epoch: 8 - 44 - mse 0.001592: - ssim 0.967402: \n","Epoch: 8 - 45 - mse 0.002164: - ssim 0.951939: \n","Epoch: 8 - 46 - mse 0.002956: - ssim 0.942018: \n","Epoch: 8 - 47 - mse 0.002120: - ssim 0.957861: \n","Epoch: 8 - 48 - mse 0.002279: - ssim 0.956826: \n","Epoch: 8 - 49 - mse 0.002426: - ssim 0.952496: \n","Epoch: 9 - 0 - mse 0.002454: - ssim 0.948339: \n","Epoch: 9 - 1 - mse 0.002123: - ssim 0.956714: \n","Epoch: 9 - 2 - mse 0.001921: - ssim 0.956285: \n","Epoch: 9 - 3 - mse 0.002492: - ssim 0.947800: \n","Epoch: 9 - 4 - mse 0.001721: - ssim 0.965966: \n","Epoch: 9 - 5 - mse 0.001472: - ssim 0.963120: \n","Epoch: 9 - 6 - mse 0.002014: - ssim 0.957710: \n","Epoch: 9 - 7 - mse 0.001578: - ssim 0.965382: \n","Epoch: 9 - 8 - mse 0.002002: - ssim 0.959294: \n","Epoch: 9 - 9 - mse 0.002353: - ssim 0.954991: \n","Epoch: 9 - 10 - mse 0.001908: - ssim 0.960740: \n","Epoch: 9 - 11 - mse 0.002413: - ssim 0.952168: \n","Epoch: 9 - 12 - mse 0.001847: - ssim 0.961394: \n","Epoch: 9 - 13 - mse 0.002374: - ssim 0.955855: \n","Epoch: 9 - 14 - mse 0.001894: - ssim 0.957419: \n","Epoch: 9 - 15 - mse 0.002204: - ssim 0.954390: \n","Epoch: 9 - 16 - mse 0.001760: - ssim 0.963675: \n","Epoch: 9 - 17 - mse 0.002162: - ssim 0.955963: \n","Epoch: 9 - 18 - mse 0.002162: - ssim 0.956183: \n","Epoch: 9 - 19 - mse 0.001577: - ssim 0.966196: \n","Epoch: 9 - 20 - mse 0.001777: - ssim 0.961171: \n","Epoch: 9 - 21 - mse 0.002327: - ssim 0.955555: \n","Epoch: 9 - 22 - mse 0.001826: - ssim 0.960721: \n","Epoch: 9 - 23 - mse 0.001753: - ssim 0.954170: \n","Epoch: 9 - 24 - mse 0.001369: - ssim 0.969995: \n","Epoch: 9 - 25 - mse 0.001698: - ssim 0.963434: \n","Epoch: 9 - 26 - mse 0.002552: - ssim 0.950170: \n","Epoch: 9 - 27 - mse 0.002372: - ssim 0.955640: \n","Epoch: 9 - 28 - mse 0.002470: - ssim 0.951931: \n","Epoch: 9 - 29 - mse 0.001756: - ssim 0.961576: \n","Epoch: 9 - 30 - mse 0.002413: - ssim 0.952803: \n","Epoch: 9 - 31 - mse 0.002309: - ssim 0.956585: \n","Epoch: 9 - 32 - mse 0.001632: - ssim 0.963950: \n","Epoch: 9 - 33 - mse 0.001845: - ssim 0.958566: \n","Epoch: 9 - 34 - mse 0.001618: - ssim 0.963084: \n","Epoch: 9 - 35 - mse 0.001904: - ssim 0.960515: \n","Epoch: 9 - 36 - mse 0.002060: - ssim 0.954752: \n","Epoch: 9 - 37 - mse 0.002069: - ssim 0.958154: \n","Epoch: 9 - 38 - mse 0.001752: - ssim 0.963927: \n","Epoch: 9 - 39 - mse 0.002345: - ssim 0.952349: \n","Epoch: 9 - 40 - mse 0.001715: - ssim 0.962897: \n","Epoch: 9 - 41 - mse 0.002186: - ssim 0.958739: \n","Epoch: 9 - 42 - mse 0.001944: - ssim 0.960645: \n","Epoch: 9 - 43 - mse 0.002021: - ssim 0.959352: \n","Epoch: 9 - 44 - mse 0.002241: - ssim 0.953269: \n","Epoch: 9 - 45 - mse 0.002007: - ssim 0.957850: \n","Epoch: 9 - 46 - mse 0.001333: - ssim 0.966122: \n","Epoch: 9 - 47 - mse 0.002266: - ssim 0.959088: \n","Epoch: 9 - 48 - mse 0.001909: - ssim 0.959921: \n","Epoch: 9 - 49 - mse 0.002203: - ssim 0.957447: \n","testing dataset\n","0.0004110973758603313\n","0.00039393045213194177\n","0.00036578615531144225\n","0.0004228370549878598\n","0.00042258335490777095\n","0.00042399991718175924\n","0.00040523874298062124\n","0.000425713031641773\n","0.00036802167568163153\n","0.00034067174233716094\n","0.00036099319228044053\n","0.0003214252544053508\n","0.0003844825947086743\n","0.00028294487577809456\n","0.00031846345716460435\n","0.000288397269284112\n","0.0002657020878718135\n","0.00026416781527062726\n","0.00030636102982208014\n","0.0002505721294984309\n","0.0002555075104825039\n","0.0003099410975040869\n","0.0003266809802100442\n","0.00031403615985213415\n","0.00036773281506923515\n","0.0003252016598710201\n","0.00027807721292497\n","0.00027825671132361633\n","0.0003522667614523995\n","0.00031952455742349735\n"],"name":"stdout"}]}]}