{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Automap.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SzJMz1D7nNZa","colab_type":"code","outputId":"e81f5a90-9fc3-4951-b29a-7996e93f37db","executionInfo":{"status":"ok","timestamp":1572284396435,"user_tz":240,"elapsed":20108,"user":{"displayName":"xd xd","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAgTHZWotfUszvYUVAYAD0KBsJyLtefel7POB-l=s64","userId":"07693667503497446418"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["#!/usr/bin/env python\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import time\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","from skimage.transform import radon, iradon\n","from skimage.measure import compare_ssim\n","import math\n","from google.colab import drive    # in ordre to mount drive to colab\n","drive.mount('/content/gdrive')\n","\n","gpu_options = tf.GPUOptions(allow_growth=True)\n","session_config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options, allow_soft_placement=True)\n","sess = tf.Session(config=session_config)\n","\n","batch_size = 10\n","learning_rate = 1.0e-4\n","width = 64       # dimension of k-space data and image data are the same\n","lambda_sl = 0.5\n","use_gan = True # change to False if not using GAN (LS_GAN).\n","if use_gan:\n","  disc_iters = 4\n","  lambda_al = 0.0025\n","else:\n","  disc_iters = 0\n","  lambda_al = 0\n","\n","num_epoch = 10"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jhwK1gmnfFk","colab_type":"code","colab":{}},"source":["def read_data(file_name, if_complex=False):\n","    if if_complex:\n","      f = h5py.File(file_name, 'r')\n","      label = np.array(f['data'], dtype=np.complex64)\n","      f.close()\n","      label = np.append(np.expand_dims(np.real(label),-1), np.expand_dims(np.imag(label), -1), -1)\n","      print(label.shape)\n","    else:\n","      f = h5py.File(file_name, 'r')\n","      label = np.array(f['data'], dtype=np.float32)\n","      f.close()\n","    return label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOEGx7k3nukw","colab_type":"code","colab":{}},"source":["    def generator(x):\n","        x = tf.reshape(x, [-1, 64 * 64 * 2])  # input contains both real and complex parts.\n","        output = tf.contrib.layers.fully_connected(x,\n","                                               64 * 64 * 2,\n","                                               activation_fn=tf.tanh,\n","                                               weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                                               biases_initializer=tf.zeros_initializer(),\n","                                               trainable=True)       # first FC layer with 64 * 64 * 2 neurons\n","        print(output)\n","        output = tf.layers.batch_normalization(output)     # batch normalization\n","                \n","        output = tf.contrib.layers.fully_connected(output,\n","                                               64 * 64,\n","                                               activation_fn=tf.tanh,\n","                                               weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                                               weights_regularizer=None,\n","                                               biases_initializer=tf.zeros_initializer(),\n","                                               trainable=True,)# second FC layer with 64 * 64 neurons\n","    \n","        output = tf.layers.batch_normalization(output) # batch normalization\n","            \n","        output = tf.reshape(output, [-1, 64, 64, 1]) # get images\n","        \n","        output = tf.layers.conv2d(output, \n","                                  filters=64, \n","                                  kernel_size=3,\n","                                  strides=1,\n","                                  padding='SAME',\n","                                  activation=tf.nn.relu,\n","                                  use_bias=True,\n","                                  bias_initializer=tf.zeros_initializer(),\n","                                  trainable=True)   # convolutional layers used to denosing images\n","    \n","        output = tf.layers.batch_normalization(output)\n","    \n","        output = tf.layers.conv2d(output, \n","                                  filters=64, \n","                                  kernel_size=3,\n","                                  strides=1,\n","                                  padding='SAME',\n","                                  activation=tf.nn.relu,\n","                                  use_bias=True,\n","                                  bias_initializer=tf.zeros_initializer(),\n","                                  trainable=True)\n","        \n","        output = tf.layers.batch_normalization(output)\n","        \n","        output = tf.layers.conv2d(output, \n","                                  filters=1, \n","                                  kernel_size=3,\n","                                  strides=1,\n","                                  padding='SAME',\n","                                  activation=tf.nn.relu,\n","                                  use_bias=True,\n","                                  bias_initializer=tf.zeros_initializer(),\n","                                  trainable=True)\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qP66vyPboBHx","colab_type":"code","colab":{}},"source":["def discriminator(x):\n","    reuse=tf.AUTO_REUSE   \n","    outputs = tf.layers.conv2d(x, 64, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1')\n","    outputs = tf.nn.relu(outputs)\n","        \n","    outputs = tf.layers.conv2d(outputs, 64, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv3')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv4')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv5')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv6')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.dense(outputs, units=1024, name='dense1')\n","    outputs = tf.nn.relu(outputs)\n","    outputs = tf.layers.dense(outputs, units=1, name='dense2')\n","    return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoV7OSJFn2NN","colab_type":"code","outputId":"5b374152-376e-4b86-fc9c-19ede7fd7d26","executionInfo":{"status":"error","timestamp":1572284597050,"user_tz":240,"elapsed":220691,"user":{"displayName":"xd xd","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAgTHZWotfUszvYUVAYAD0KBsJyLtefel7POB-l=s64","userId":"07693667503497446418"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["lr = tf.placeholder(dtype=tf.float32, shape=[])  # learning rate\n","real_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, width, width, 1], name='real')  # placeholder for images\n","fake_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, width, width, 2], name='fake')  # placeholder for k-space data (complex). one channel contains real part\n","                                                                                                       # and the other channel contains the complex part\n","with tf.variable_scope('generator_model', reuse=tf.AUTO_REUSE) as scope_generator_model:\n","    Gz = generator(fake_placeholder)                                 # feed k-space data into the network\n","print(Gz)\n","with tf.variable_scope('discriminator_model', reuse=tf.AUTO_REUSE) as scope_discriminator_model:\n","    FD_d = discriminator(real_placeholder)                   # same discriminator network as in the autoencoder\n","    LD_d = discriminator(Gz)\n","print(FD_d)\n","print(LD_d)\n","\n","\n","disc_loss = tf.reduce_mean((FD_d - 1)**2 + (LD_d - 0)**2)\n","disc_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator_model')\n","d_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(disc_loss, var_list=disc_variables)\n","# training discriminator\n","\n","ssim = tf.squeeze(tf.reduce_mean(tf.image.ssim(Gz, real_placeholder, 1.0)))\n","ssim_loss = 1 - ssim\n","mse = tf.reduce_mean(tf.squared_difference(Gz, real_placeholder))\n","gen_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_model')\n","gen_cost = tf.reduce_mean((LD_d - 1)**2)\n","\n","g_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(mse + lambda_al * gen_cost + lambda_sl * ssim_loss, var_list=gen_variables)\n","# training generator\n","\n","saver = tf.train.Saver()\n","print(\"enter Session\")\n","sess.run(tf.global_variables_initializer())  # initializing parameters\n","#saver.restore(sess, '/content/gdrive/My Drive/hands_on4/Automap/model_dm/SIGmathmodel100.ckpt')   # used to restore medels\n","print('model restored')\n","print(\"Begin training\")\n","label = read_data('/content/gdrive/My Drive/hands_on4/Automap/Train/Label.hdf5')   # read label\n","data = read_data('/content/gdrive/My Drive/hands_on4/Automap/Train/Data.hdf5', if_complex=True)  # read data\n","\n","\n","for iteration in range(num_epoch):\n","    #break\n","    label, data = shuffle(label, data)\n","    val_lr = learning_rate / 2\n","    num_batches = data.shape[0] // batch_size\n","    for i in range(num_batches):\n","        for _ in range(disc_iters):\n","            idx = np.random.permutation(label.shape[0])   # input to the discriminator and the input to the generator should be different\n","            batch_label = label[idx[:batch_size]]\n","            batch_label = np.expand_dims(batch_label, -1)\n","            \n","            batch_data = data[idx[:batch_size]]\n","            \n","            _ = sess.run(d_trainer, feed_dict={real_placeholder: batch_label,\n","                                               fake_placeholder: batch_data,\n","                                               lr: val_lr})  # training discriminator\n","            \n","        batch_label = label[i * batch_size: (i + 1) * batch_size] # input to the discriminator and the input to the generator should be different\n","        batch_label = np.expand_dims(batch_label, -1)\n","        batch_data = data[i * batch_size: (i + 1) * batch_size]\n","        _mse, _ssim, _ = sess.run([mse, ssim, g_trainer], feed_dict={real_placeholder: (batch_label),\n","                                                                             fake_placeholder: (batch_data), lr: val_lr})\n","        # training generator\n","\n","        print('Epoch: %d - %d - mse %.6f: - ssim %.6f: ' % (iteration, i, _mse, _ssim))\n","\n","saver.save(sess, '/content/gdrive/My Drive/hands_on4/Automap/model_dm/SIGmathmodel100.ckpt')\n","  \n","print(\"testing dataset\")\n","test_label = read_data('/content/gdrive/My Drive/hands_on4/Automap/Train/Label.hdf5')      # read label  \n","test_data = read_data('/content/gdrive/My Drive/hands_on4/Automap/Train/Data.hdf5', if_complex=True)  # read data\n","for c in range(test_label.shape[0] // batch_size):\n","    test_batch_label = test_label[c * batch_size: (c + 1) * batch_size]\n","    test_batch_label = np.expand_dims(test_batch_label, -1)\n","    \n","    test_batch_data = test_data[c * batch_size: (c + 1) * batch_size]\n","    \n","    with tf.variable_scope('generator_model') as scope:\n","        scope.reuse_variables()\n","        estimated = sess.run(Gz, feed_dict={fake_placeholder: test_batch_data})\n","    for i in range(batch_size):\n","      t_r = np.squeeze(test_batch_label[i])\n","      e_1 = np.squeeze(estimated[i])\n","\n","      f_n = '/content/gdrive/My Drive/hands_on4/Automap/testing_results/' + str(c) + str(i) + 'validation.jpg'  # save reconstructed images\n","      plt.imsave(f_n, e_1, cmap='gray')\n","      f_n = '/content/gdrive/My Drive/hands_on4/Automap/testing_results/' + str(c) + str(i) + 'real_.jpg'  # save real images\n","      plt.imsave(f_n, t_r, cmap='gray')\n","\n","      z = (t_r - e_1) ** 2\n","      print(np.mean(z))   # print mse loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","Tensor(\"generator_model/fully_connected/Tanh:0\", shape=(10, 8192), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-3-e8567cbd65fb>:10: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","WARNING:tensorflow:From <ipython-input-3-e8567cbd65fb>:32: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv2D` instead.\n","Tensor(\"generator_model/conv2d_2/Relu:0\", shape=(10, 64, 64, 1), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-4-7025c65a4d15>:21: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","Tensor(\"discriminator_model/dense2/BiasAdd:0\", shape=(10, 8, 8, 1), dtype=float32)\n","Tensor(\"discriminator_model/dense2_1/BiasAdd:0\", shape=(10, 8, 8, 1), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","enter Session\n","model restored\n","Begin training\n","(500, 64, 64, 2)\n","Epoch: 0 - 0 - mse 0.027867: - ssim 0.052533: \n","Epoch: 0 - 1 - mse 0.025983: - ssim 0.068894: \n","Epoch: 0 - 2 - mse 0.028475: - ssim 0.080457: \n","Epoch: 0 - 3 - mse 0.026871: - ssim 0.094927: \n","Epoch: 0 - 4 - mse 0.028651: - ssim 0.092479: \n","Epoch: 0 - 5 - mse 0.023572: - ssim 0.117581: \n","Epoch: 0 - 6 - mse 0.028332: - ssim 0.120403: \n","Epoch: 0 - 7 - mse 0.028889: - ssim 0.142034: \n","Epoch: 0 - 8 - mse 0.030299: - ssim 0.168110: \n","Epoch: 0 - 9 - mse 0.026753: - ssim 0.197978: \n","Epoch: 0 - 10 - mse 0.030180: - ssim 0.215232: \n","Epoch: 0 - 11 - mse 0.029413: - ssim 0.244451: \n","Epoch: 0 - 12 - mse 0.028835: - ssim 0.256258: \n","Epoch: 0 - 13 - mse 0.032441: - ssim 0.281384: \n","Epoch: 0 - 14 - mse 0.032903: - ssim 0.250806: \n","Epoch: 0 - 15 - mse 0.030040: - ssim 0.272868: \n","Epoch: 0 - 16 - mse 0.028643: - ssim 0.280974: \n","Epoch: 0 - 17 - mse 0.030558: - ssim 0.260750: \n","Epoch: 0 - 18 - mse 0.027580: - ssim 0.280645: \n","Epoch: 0 - 19 - mse 0.029073: - ssim 0.267191: \n","Epoch: 0 - 20 - mse 0.032233: - ssim 0.274808: \n","Epoch: 0 - 21 - mse 0.025910: - ssim 0.296883: \n","Epoch: 0 - 22 - mse 0.024006: - ssim 0.307876: \n","Epoch: 0 - 23 - mse 0.024727: - ssim 0.325970: \n","Epoch: 0 - 24 - mse 0.026045: - ssim 0.323618: \n","Epoch: 0 - 25 - mse 0.022396: - ssim 0.317031: \n","Epoch: 0 - 26 - mse 0.021638: - ssim 0.334713: \n","Epoch: 0 - 27 - mse 0.022781: - ssim 0.312046: \n","Epoch: 0 - 28 - mse 0.020067: - ssim 0.356209: \n","Epoch: 0 - 29 - mse 0.022717: - ssim 0.346784: \n","Epoch: 0 - 30 - mse 0.021618: - ssim 0.351686: \n","Epoch: 0 - 31 - mse 0.023219: - ssim 0.319112: \n","Epoch: 0 - 32 - mse 0.019484: - ssim 0.381556: \n","Epoch: 0 - 33 - mse 0.016487: - ssim 0.364238: \n","Epoch: 0 - 34 - mse 0.014902: - ssim 0.388116: \n","Epoch: 0 - 35 - mse 0.016761: - ssim 0.389309: \n","Epoch: 0 - 36 - mse 0.016293: - ssim 0.368169: \n","Epoch: 0 - 37 - mse 0.014576: - ssim 0.397983: \n","Epoch: 0 - 38 - mse 0.015311: - ssim 0.398017: \n","Epoch: 0 - 39 - mse 0.019562: - ssim 0.396908: \n","Epoch: 0 - 40 - mse 0.011858: - ssim 0.405512: \n","Epoch: 0 - 41 - mse 0.013186: - ssim 0.420736: \n","Epoch: 0 - 42 - mse 0.016346: - ssim 0.399749: \n","Epoch: 0 - 43 - mse 0.014875: - ssim 0.394715: \n","Epoch: 0 - 44 - mse 0.016042: - ssim 0.394765: \n","Epoch: 0 - 45 - mse 0.010938: - ssim 0.439979: \n","Epoch: 0 - 46 - mse 0.013150: - ssim 0.424529: \n","Epoch: 0 - 47 - mse 0.011531: - ssim 0.433960: \n","Epoch: 0 - 48 - mse 0.012283: - ssim 0.438703: \n","Epoch: 0 - 49 - mse 0.011755: - ssim 0.431907: \n","Epoch: 1 - 0 - mse 0.012079: - ssim 0.452614: \n","Epoch: 1 - 1 - mse 0.010733: - ssim 0.474860: \n","Epoch: 1 - 2 - mse 0.011699: - ssim 0.481918: \n","Epoch: 1 - 3 - mse 0.012337: - ssim 0.439544: \n","Epoch: 1 - 4 - mse 0.011424: - ssim 0.482185: \n","Epoch: 1 - 5 - mse 0.010820: - ssim 0.488054: \n","Epoch: 1 - 6 - mse 0.010360: - ssim 0.466180: \n","Epoch: 1 - 7 - mse 0.008953: - ssim 0.487168: \n","Epoch: 1 - 8 - mse 0.010476: - ssim 0.475279: \n","Epoch: 1 - 9 - mse 0.010309: - ssim 0.507153: \n","Epoch: 1 - 10 - mse 0.008660: - ssim 0.521706: \n","Epoch: 1 - 11 - mse 0.009306: - ssim 0.504038: \n","Epoch: 1 - 12 - mse 0.009537: - ssim 0.510839: \n","Epoch: 1 - 13 - mse 0.011157: - ssim 0.493019: \n","Epoch: 1 - 14 - mse 0.009231: - ssim 0.513321: \n","Epoch: 1 - 15 - mse 0.009715: - ssim 0.495798: \n","Epoch: 1 - 16 - mse 0.012273: - ssim 0.486281: \n","Epoch: 1 - 17 - mse 0.010532: - ssim 0.516573: \n","Epoch: 1 - 18 - mse 0.011241: - ssim 0.493034: \n","Epoch: 1 - 19 - mse 0.010504: - ssim 0.513323: \n","Epoch: 1 - 20 - mse 0.011054: - ssim 0.513554: \n","Epoch: 1 - 21 - mse 0.010847: - ssim 0.526812: \n","Epoch: 1 - 22 - mse 0.008974: - ssim 0.554377: \n","Epoch: 1 - 23 - mse 0.009944: - ssim 0.534978: \n","Epoch: 1 - 24 - mse 0.008731: - ssim 0.558425: \n","Epoch: 1 - 25 - mse 0.010768: - ssim 0.534877: \n","Epoch: 1 - 26 - mse 0.008550: - ssim 0.539751: \n","Epoch: 1 - 27 - mse 0.009969: - ssim 0.527241: \n","Epoch: 1 - 28 - mse 0.010456: - ssim 0.552218: \n","Epoch: 1 - 29 - mse 0.009078: - ssim 0.546970: \n","Epoch: 1 - 30 - mse 0.008544: - ssim 0.562698: \n","Epoch: 1 - 31 - mse 0.008447: - ssim 0.550909: \n","Epoch: 1 - 32 - mse 0.009930: - ssim 0.559236: \n","Epoch: 1 - 33 - mse 0.010514: - ssim 0.546696: \n","Epoch: 1 - 34 - mse 0.008680: - ssim 0.557100: \n","Epoch: 1 - 35 - mse 0.008475: - ssim 0.560758: \n","Epoch: 1 - 36 - mse 0.010386: - ssim 0.546678: \n","Epoch: 1 - 37 - mse 0.008543: - ssim 0.572774: \n","Epoch: 1 - 38 - mse 0.007717: - ssim 0.572720: \n","Epoch: 1 - 39 - mse 0.007955: - ssim 0.588059: \n","Epoch: 1 - 40 - mse 0.008343: - ssim 0.574883: \n","Epoch: 1 - 41 - mse 0.009525: - ssim 0.567268: \n","Epoch: 1 - 42 - mse 0.007358: - ssim 0.593195: \n","Epoch: 1 - 43 - mse 0.009377: - ssim 0.559132: \n","Epoch: 1 - 44 - mse 0.008046: - ssim 0.572340: \n","Epoch: 1 - 45 - mse 0.008142: - ssim 0.592321: \n","Epoch: 1 - 46 - mse 0.008279: - ssim 0.572455: \n","Epoch: 1 - 47 - mse 0.008879: - ssim 0.585071: \n","Epoch: 1 - 48 - mse 0.008430: - ssim 0.592766: \n","Epoch: 1 - 49 - mse 0.007771: - ssim 0.595628: \n","Epoch: 2 - 0 - mse 0.007967: - ssim 0.615145: \n","Epoch: 2 - 1 - mse 0.008098: - ssim 0.612674: \n","Epoch: 2 - 2 - mse 0.007953: - ssim 0.614980: \n","Epoch: 2 - 3 - mse 0.007382: - ssim 0.612171: \n","Epoch: 2 - 4 - mse 0.007522: - ssim 0.628037: \n","Epoch: 2 - 5 - mse 0.008123: - ssim 0.624523: \n","Epoch: 2 - 6 - mse 0.007503: - ssim 0.625208: \n","Epoch: 2 - 7 - mse 0.007019: - ssim 0.633450: \n","Epoch: 2 - 8 - mse 0.007864: - ssim 0.615147: \n","Epoch: 2 - 9 - mse 0.007585: - ssim 0.620080: \n","Epoch: 2 - 10 - mse 0.008027: - ssim 0.612283: \n","Epoch: 2 - 11 - mse 0.007877: - ssim 0.634954: \n","Epoch: 2 - 12 - mse 0.007835: - ssim 0.627448: \n","Epoch: 2 - 13 - mse 0.007417: - ssim 0.631674: \n","Epoch: 2 - 14 - mse 0.007674: - ssim 0.630600: \n","Epoch: 2 - 15 - mse 0.008292: - ssim 0.619045: \n","Epoch: 2 - 16 - mse 0.006415: - ssim 0.646886: \n","Epoch: 2 - 17 - mse 0.007298: - ssim 0.636090: \n","Epoch: 2 - 18 - mse 0.006857: - ssim 0.645388: \n","Epoch: 2 - 19 - mse 0.006970: - ssim 0.649387: \n","Epoch: 2 - 20 - mse 0.007548: - ssim 0.659588: \n","Epoch: 2 - 21 - mse 0.007801: - ssim 0.635208: \n","Epoch: 2 - 22 - mse 0.006385: - ssim 0.648578: \n","Epoch: 2 - 23 - mse 0.006663: - ssim 0.662400: \n","Epoch: 2 - 24 - mse 0.006489: - ssim 0.654648: \n","Epoch: 2 - 25 - mse 0.006936: - ssim 0.646885: \n","Epoch: 2 - 26 - mse 0.007151: - ssim 0.638273: \n","Epoch: 2 - 27 - mse 0.007691: - ssim 0.644847: \n","Epoch: 2 - 28 - mse 0.007066: - ssim 0.651492: \n","Epoch: 2 - 29 - mse 0.006659: - ssim 0.658525: \n","Epoch: 2 - 30 - mse 0.005919: - ssim 0.678629: \n","Epoch: 2 - 31 - mse 0.008100: - ssim 0.632974: \n","Epoch: 2 - 32 - mse 0.007012: - ssim 0.661791: \n","Epoch: 2 - 33 - mse 0.006296: - ssim 0.668154: \n","Epoch: 2 - 34 - mse 0.007759: - ssim 0.665522: \n","Epoch: 2 - 35 - mse 0.007546: - ssim 0.651541: \n","Epoch: 2 - 36 - mse 0.006061: - ssim 0.682377: \n","Epoch: 2 - 37 - mse 0.007154: - ssim 0.656520: \n","Epoch: 2 - 38 - mse 0.006811: - ssim 0.672586: \n","Epoch: 2 - 39 - mse 0.007560: - ssim 0.664115: \n","Epoch: 2 - 40 - mse 0.005937: - ssim 0.679908: \n","Epoch: 2 - 41 - mse 0.005951: - ssim 0.670059: \n","Epoch: 2 - 42 - mse 0.006889: - ssim 0.663843: \n","Epoch: 2 - 43 - mse 0.006819: - ssim 0.657261: \n","Epoch: 2 - 44 - mse 0.005840: - ssim 0.672912: \n","Epoch: 2 - 45 - mse 0.007265: - ssim 0.647116: \n","Epoch: 2 - 46 - mse 0.006872: - ssim 0.663979: \n","Epoch: 2 - 47 - mse 0.006737: - ssim 0.678694: \n","Epoch: 2 - 48 - mse 0.007035: - ssim 0.669986: \n","Epoch: 2 - 49 - mse 0.006490: - ssim 0.685605: \n","Epoch: 3 - 0 - mse 0.005658: - ssim 0.695961: \n","Epoch: 3 - 1 - mse 0.005686: - ssim 0.709413: \n","Epoch: 3 - 2 - mse 0.006116: - ssim 0.701577: \n","Epoch: 3 - 3 - mse 0.006190: - ssim 0.699289: \n","Epoch: 3 - 4 - mse 0.006362: - ssim 0.692842: \n","Epoch: 3 - 5 - mse 0.006336: - ssim 0.685391: \n","Epoch: 3 - 6 - mse 0.006258: - ssim 0.685493: \n","Epoch: 3 - 7 - mse 0.006527: - ssim 0.686570: \n","Epoch: 3 - 8 - mse 0.006317: - ssim 0.706379: \n","Epoch: 3 - 9 - mse 0.005949: - ssim 0.700226: \n","Epoch: 3 - 10 - mse 0.005487: - ssim 0.713224: \n","Epoch: 3 - 11 - mse 0.006651: - ssim 0.693744: \n","Epoch: 3 - 12 - mse 0.006054: - ssim 0.688513: \n","Epoch: 3 - 13 - mse 0.005779: - ssim 0.694537: \n","Epoch: 3 - 14 - mse 0.005753: - ssim 0.706937: \n","Epoch: 3 - 15 - mse 0.006789: - ssim 0.685517: \n","Epoch: 3 - 16 - mse 0.005424: - ssim 0.714939: \n","Epoch: 3 - 17 - mse 0.006697: - ssim 0.705724: \n","Epoch: 3 - 18 - mse 0.005666: - ssim 0.712434: \n","Epoch: 3 - 19 - mse 0.005479: - ssim 0.712193: \n","Epoch: 3 - 20 - mse 0.006236: - ssim 0.708669: \n","Epoch: 3 - 21 - mse 0.005857: - ssim 0.706298: \n","Epoch: 3 - 22 - mse 0.005612: - ssim 0.715272: \n","Epoch: 3 - 23 - mse 0.005867: - ssim 0.711799: \n","Epoch: 3 - 24 - mse 0.006202: - ssim 0.697075: \n","Epoch: 3 - 25 - mse 0.005632: - ssim 0.720623: \n","Epoch: 3 - 26 - mse 0.005472: - ssim 0.705595: \n","Epoch: 3 - 27 - mse 0.005505: - ssim 0.721822: \n","Epoch: 3 - 28 - mse 0.006463: - ssim 0.695569: \n","Epoch: 3 - 29 - mse 0.005326: - ssim 0.741901: \n","Epoch: 3 - 30 - mse 0.005588: - ssim 0.719809: \n","Epoch: 3 - 31 - mse 0.005479: - ssim 0.714823: \n","Epoch: 3 - 32 - mse 0.005861: - ssim 0.701319: \n","Epoch: 3 - 33 - mse 0.006314: - ssim 0.697652: \n","Epoch: 3 - 34 - mse 0.006373: - ssim 0.707928: \n","Epoch: 3 - 35 - mse 0.005696: - ssim 0.710941: \n","Epoch: 3 - 36 - mse 0.005860: - ssim 0.715596: \n","Epoch: 3 - 37 - mse 0.006436: - ssim 0.720625: \n","Epoch: 3 - 38 - mse 0.006205: - ssim 0.721218: \n","Epoch: 3 - 39 - mse 0.005508: - ssim 0.709596: \n","Epoch: 3 - 40 - mse 0.005421: - ssim 0.711465: \n","Epoch: 3 - 41 - mse 0.005283: - ssim 0.723518: \n","Epoch: 3 - 42 - mse 0.005310: - ssim 0.717915: \n","Epoch: 3 - 43 - mse 0.005599: - ssim 0.724898: \n","Epoch: 3 - 44 - mse 0.005767: - ssim 0.721920: \n","Epoch: 3 - 45 - mse 0.005441: - ssim 0.731792: \n","Epoch: 3 - 46 - mse 0.005606: - ssim 0.723874: \n","Epoch: 3 - 47 - mse 0.005175: - ssim 0.716729: \n","Epoch: 3 - 48 - mse 0.004819: - ssim 0.733778: \n","Epoch: 3 - 49 - mse 0.005152: - ssim 0.735763: \n","Epoch: 4 - 0 - mse 0.005257: - ssim 0.754540: \n","Epoch: 4 - 1 - mse 0.005564: - ssim 0.738352: \n","Epoch: 4 - 2 - mse 0.004979: - ssim 0.735771: \n","Epoch: 4 - 3 - mse 0.004522: - ssim 0.744750: \n","Epoch: 4 - 4 - mse 0.005183: - ssim 0.738882: \n","Epoch: 4 - 5 - mse 0.005164: - ssim 0.740966: \n","Epoch: 4 - 6 - mse 0.005281: - ssim 0.741836: \n","Epoch: 4 - 7 - mse 0.004974: - ssim 0.739430: \n","Epoch: 4 - 8 - mse 0.004974: - ssim 0.741094: \n","Epoch: 4 - 9 - mse 0.004584: - ssim 0.757080: \n","Epoch: 4 - 10 - mse 0.004885: - ssim 0.751736: \n","Epoch: 4 - 11 - mse 0.005375: - ssim 0.741840: \n","Epoch: 4 - 12 - mse 0.005755: - ssim 0.731038: \n","Epoch: 4 - 13 - mse 0.004973: - ssim 0.737231: \n","Epoch: 4 - 14 - mse 0.004412: - ssim 0.749090: \n","Epoch: 4 - 15 - mse 0.005182: - ssim 0.729450: \n","Epoch: 4 - 16 - mse 0.004715: - ssim 0.757572: \n","Epoch: 4 - 17 - mse 0.004739: - ssim 0.744089: \n","Epoch: 4 - 18 - mse 0.005760: - ssim 0.737422: \n","Epoch: 4 - 19 - mse 0.004990: - ssim 0.744222: \n","Epoch: 4 - 20 - mse 0.004463: - ssim 0.757559: \n","Epoch: 4 - 21 - mse 0.005281: - ssim 0.747748: \n","Epoch: 4 - 22 - mse 0.004742: - ssim 0.754858: \n","Epoch: 4 - 23 - mse 0.004808: - ssim 0.750423: \n","Epoch: 4 - 24 - mse 0.004928: - ssim 0.750077: \n","Epoch: 4 - 25 - mse 0.004924: - ssim 0.753192: \n","Epoch: 4 - 26 - mse 0.005250: - ssim 0.754876: \n","Epoch: 4 - 27 - mse 0.004694: - ssim 0.757558: \n","Epoch: 4 - 28 - mse 0.004922: - ssim 0.746199: \n","Epoch: 4 - 29 - mse 0.005091: - ssim 0.741080: \n","Epoch: 4 - 30 - mse 0.005451: - ssim 0.753067: \n","Epoch: 4 - 31 - mse 0.005243: - ssim 0.757126: \n","Epoch: 4 - 32 - mse 0.005247: - ssim 0.736865: \n","Epoch: 4 - 33 - mse 0.004641: - ssim 0.741632: \n","Epoch: 4 - 34 - mse 0.004802: - ssim 0.756266: \n","Epoch: 4 - 35 - mse 0.005327: - ssim 0.731088: \n","Epoch: 4 - 36 - mse 0.005229: - ssim 0.751006: \n","Epoch: 4 - 37 - mse 0.005400: - ssim 0.734523: \n","Epoch: 4 - 38 - mse 0.004837: - ssim 0.741001: \n","Epoch: 4 - 39 - mse 0.005350: - ssim 0.745275: \n","Epoch: 4 - 40 - mse 0.004919: - ssim 0.753845: \n","Epoch: 4 - 41 - mse 0.004393: - ssim 0.766969: \n","Epoch: 4 - 42 - mse 0.004893: - ssim 0.748469: \n","Epoch: 4 - 43 - mse 0.005161: - ssim 0.741248: \n","Epoch: 4 - 44 - mse 0.005003: - ssim 0.746076: \n","Epoch: 4 - 45 - mse 0.004642: - ssim 0.764008: \n","Epoch: 4 - 46 - mse 0.004517: - ssim 0.768847: \n","Epoch: 4 - 47 - mse 0.004525: - ssim 0.758713: \n","Epoch: 4 - 48 - mse 0.005096: - ssim 0.753392: \n","Epoch: 4 - 49 - mse 0.004928: - ssim 0.762280: \n","Epoch: 5 - 0 - mse 0.004035: - ssim 0.774145: \n","Epoch: 5 - 1 - mse 0.004073: - ssim 0.783259: \n","Epoch: 5 - 2 - mse 0.004498: - ssim 0.772186: \n","Epoch: 5 - 3 - mse 0.004049: - ssim 0.778610: \n","Epoch: 5 - 4 - mse 0.004148: - ssim 0.777131: \n","Epoch: 5 - 5 - mse 0.004430: - ssim 0.772115: \n","Epoch: 5 - 6 - mse 0.004443: - ssim 0.770057: \n","Epoch: 5 - 7 - mse 0.005404: - ssim 0.758919: \n","Epoch: 5 - 8 - mse 0.004017: - ssim 0.776594: \n","Epoch: 5 - 9 - mse 0.004320: - ssim 0.769125: \n","Epoch: 5 - 10 - mse 0.004558: - ssim 0.769122: \n","Epoch: 5 - 11 - mse 0.004943: - ssim 0.763158: \n","Epoch: 5 - 12 - mse 0.004328: - ssim 0.778492: \n","Epoch: 5 - 13 - mse 0.004320: - ssim 0.774014: \n","Epoch: 5 - 14 - mse 0.005256: - ssim 0.747304: \n","Epoch: 5 - 15 - mse 0.004433: - ssim 0.766802: \n","Epoch: 5 - 16 - mse 0.004794: - ssim 0.766312: \n","Epoch: 5 - 17 - mse 0.004395: - ssim 0.758372: \n","Epoch: 5 - 18 - mse 0.004844: - ssim 0.770076: \n","Epoch: 5 - 19 - mse 0.004724: - ssim 0.762178: \n","Epoch: 5 - 20 - mse 0.004596: - ssim 0.773726: \n","Epoch: 5 - 21 - mse 0.004054: - ssim 0.780301: \n","Epoch: 5 - 22 - mse 0.004545: - ssim 0.769729: \n","Epoch: 5 - 23 - mse 0.004028: - ssim 0.778512: \n","Epoch: 5 - 24 - mse 0.004437: - ssim 0.766374: \n","Epoch: 5 - 25 - mse 0.004911: - ssim 0.772070: \n","Epoch: 5 - 26 - mse 0.005019: - ssim 0.769716: \n","Epoch: 5 - 27 - mse 0.004154: - ssim 0.763969: \n","Epoch: 5 - 28 - mse 0.004518: - ssim 0.765035: \n","Epoch: 5 - 29 - mse 0.004736: - ssim 0.771930: \n","Epoch: 5 - 30 - mse 0.004835: - ssim 0.766730: \n","Epoch: 5 - 31 - mse 0.004394: - ssim 0.782884: \n","Epoch: 5 - 32 - mse 0.003989: - ssim 0.778969: \n","Epoch: 5 - 33 - mse 0.004389: - ssim 0.763837: \n","Epoch: 5 - 34 - mse 0.004658: - ssim 0.776285: \n","Epoch: 5 - 35 - mse 0.004366: - ssim 0.774390: \n","Epoch: 5 - 36 - mse 0.004545: - ssim 0.776840: \n","Epoch: 5 - 37 - mse 0.004624: - ssim 0.778216: \n","Epoch: 5 - 38 - mse 0.004181: - ssim 0.788141: \n","Epoch: 5 - 39 - mse 0.004696: - ssim 0.767346: \n","Epoch: 5 - 40 - mse 0.004725: - ssim 0.780809: \n","Epoch: 5 - 41 - mse 0.004507: - ssim 0.772857: \n","Epoch: 5 - 42 - mse 0.004168: - ssim 0.785170: \n","Epoch: 5 - 43 - mse 0.004399: - ssim 0.768045: \n","Epoch: 5 - 44 - mse 0.004274: - ssim 0.771006: \n","Epoch: 5 - 45 - mse 0.004498: - ssim 0.778146: \n","Epoch: 5 - 46 - mse 0.004270: - ssim 0.784602: \n","Epoch: 5 - 47 - mse 0.004646: - ssim 0.790014: \n","Epoch: 5 - 48 - mse 0.003905: - ssim 0.780797: \n","Epoch: 5 - 49 - mse 0.003929: - ssim 0.784783: \n","Epoch: 6 - 0 - mse 0.003843: - ssim 0.791159: \n","Epoch: 6 - 1 - mse 0.004146: - ssim 0.789487: \n","Epoch: 6 - 2 - mse 0.004342: - ssim 0.778897: \n","Epoch: 6 - 3 - mse 0.004030: - ssim 0.795432: \n","Epoch: 6 - 4 - mse 0.003772: - ssim 0.803447: \n","Epoch: 6 - 5 - mse 0.004144: - ssim 0.795911: \n","Epoch: 6 - 6 - mse 0.004406: - ssim 0.789006: \n","Epoch: 6 - 7 - mse 0.004863: - ssim 0.787033: \n","Epoch: 6 - 8 - mse 0.003977: - ssim 0.784206: \n","Epoch: 6 - 9 - mse 0.004569: - ssim 0.779449: \n","Epoch: 6 - 10 - mse 0.004586: - ssim 0.779240: \n","Epoch: 6 - 11 - mse 0.004655: - ssim 0.785335: \n","Epoch: 6 - 12 - mse 0.004305: - ssim 0.790249: \n","Epoch: 6 - 13 - mse 0.004166: - ssim 0.785521: \n","Epoch: 6 - 14 - mse 0.004117: - ssim 0.789865: \n","Epoch: 6 - 15 - mse 0.003657: - ssim 0.791860: \n","Epoch: 6 - 16 - mse 0.003848: - ssim 0.803071: \n","Epoch: 6 - 17 - mse 0.003422: - ssim 0.804061: \n","Epoch: 6 - 18 - mse 0.004252: - ssim 0.787372: \n","Epoch: 6 - 19 - mse 0.003566: - ssim 0.792303: \n","Epoch: 6 - 20 - mse 0.003774: - ssim 0.790277: \n","Epoch: 6 - 21 - mse 0.003930: - ssim 0.799863: \n","Epoch: 6 - 22 - mse 0.004160: - ssim 0.795316: \n","Epoch: 6 - 23 - mse 0.003890: - ssim 0.795547: \n","Epoch: 6 - 24 - mse 0.004498: - ssim 0.780296: \n","Epoch: 6 - 25 - mse 0.003936: - ssim 0.790753: \n","Epoch: 6 - 26 - mse 0.004381: - ssim 0.785203: \n","Epoch: 6 - 27 - mse 0.004047: - ssim 0.795656: \n","Epoch: 6 - 28 - mse 0.004591: - ssim 0.791464: \n","Epoch: 6 - 29 - mse 0.003614: - ssim 0.785869: \n","Epoch: 6 - 30 - mse 0.004162: - ssim 0.792195: \n","Epoch: 6 - 31 - mse 0.004053: - ssim 0.789224: \n","Epoch: 6 - 32 - mse 0.003699: - ssim 0.796664: \n","Epoch: 6 - 33 - mse 0.004075: - ssim 0.790566: \n","Epoch: 6 - 34 - mse 0.004192: - ssim 0.790811: \n","Epoch: 6 - 35 - mse 0.004155: - ssim 0.790200: \n","Epoch: 6 - 36 - mse 0.003831: - ssim 0.792123: \n","Epoch: 6 - 37 - mse 0.003544: - ssim 0.803460: \n","Epoch: 6 - 38 - mse 0.004049: - ssim 0.787990: \n","Epoch: 6 - 39 - mse 0.004113: - ssim 0.789656: \n","Epoch: 6 - 40 - mse 0.004145: - ssim 0.787656: \n","Epoch: 6 - 41 - mse 0.004015: - ssim 0.792015: \n","Epoch: 6 - 42 - mse 0.003962: - ssim 0.796928: \n","Epoch: 6 - 43 - mse 0.003758: - ssim 0.801738: \n","Epoch: 6 - 44 - mse 0.003533: - ssim 0.796356: \n","Epoch: 6 - 45 - mse 0.003977: - ssim 0.788610: \n","Epoch: 6 - 46 - mse 0.004612: - ssim 0.790840: \n","Epoch: 6 - 47 - mse 0.003937: - ssim 0.787539: \n","Epoch: 6 - 48 - mse 0.004130: - ssim 0.792427: \n","Epoch: 6 - 49 - mse 0.004081: - ssim 0.786115: \n","Epoch: 7 - 0 - mse 0.003506: - ssim 0.810567: \n","Epoch: 7 - 1 - mse 0.004285: - ssim 0.799395: \n","Epoch: 7 - 2 - mse 0.003722: - ssim 0.793149: \n","Epoch: 7 - 3 - mse 0.004095: - ssim 0.802955: \n","Epoch: 7 - 4 - mse 0.003601: - ssim 0.803795: \n","Epoch: 7 - 5 - mse 0.003749: - ssim 0.798752: \n","Epoch: 7 - 6 - mse 0.004092: - ssim 0.801026: \n","Epoch: 7 - 7 - mse 0.003624: - ssim 0.800509: \n","Epoch: 7 - 8 - mse 0.003276: - ssim 0.818765: \n","Epoch: 7 - 9 - mse 0.004657: - ssim 0.785547: \n","Epoch: 7 - 10 - mse 0.003292: - ssim 0.805141: \n","Epoch: 7 - 11 - mse 0.004053: - ssim 0.806208: \n","Epoch: 7 - 12 - mse 0.004083: - ssim 0.802200: \n","Epoch: 7 - 13 - mse 0.003770: - ssim 0.796822: \n","Epoch: 7 - 14 - mse 0.003378: - ssim 0.813671: \n","Epoch: 7 - 15 - mse 0.003534: - ssim 0.814145: \n","Epoch: 7 - 16 - mse 0.003669: - ssim 0.811311: \n","Epoch: 7 - 17 - mse 0.004030: - ssim 0.802545: \n","Epoch: 7 - 18 - mse 0.004049: - ssim 0.800764: \n","Epoch: 7 - 19 - mse 0.003541: - ssim 0.799886: \n","Epoch: 7 - 20 - mse 0.003870: - ssim 0.789299: \n","Epoch: 7 - 21 - mse 0.003432: - ssim 0.814309: \n","Epoch: 7 - 22 - mse 0.003752: - ssim 0.804436: \n","Epoch: 7 - 23 - mse 0.003825: - ssim 0.801039: \n","Epoch: 7 - 24 - mse 0.004694: - ssim 0.793087: \n","Epoch: 7 - 25 - mse 0.003340: - ssim 0.810567: \n","Epoch: 7 - 26 - mse 0.003297: - ssim 0.809361: \n","Epoch: 7 - 27 - mse 0.004175: - ssim 0.788847: \n","Epoch: 7 - 28 - mse 0.004083: - ssim 0.803329: \n","Epoch: 7 - 29 - mse 0.003841: - ssim 0.808009: \n","Epoch: 7 - 30 - mse 0.003762: - ssim 0.807466: \n","Epoch: 7 - 31 - mse 0.003732: - ssim 0.802153: \n","Epoch: 7 - 32 - mse 0.003470: - ssim 0.805333: \n","Epoch: 7 - 33 - mse 0.003908: - ssim 0.809965: \n","Epoch: 7 - 34 - mse 0.003757: - ssim 0.805070: \n","Epoch: 7 - 35 - mse 0.003926: - ssim 0.805130: \n","Epoch: 7 - 36 - mse 0.003448: - ssim 0.805180: \n","Epoch: 7 - 37 - mse 0.004063: - ssim 0.788983: \n","Epoch: 7 - 38 - mse 0.004034: - ssim 0.808444: \n","Epoch: 7 - 39 - mse 0.003803: - ssim 0.813420: \n","Epoch: 7 - 40 - mse 0.003388: - ssim 0.819563: \n","Epoch: 7 - 41 - mse 0.003452: - ssim 0.817563: \n","Epoch: 7 - 42 - mse 0.004093: - ssim 0.803742: \n","Epoch: 7 - 43 - mse 0.003284: - ssim 0.816226: \n","Epoch: 7 - 44 - mse 0.003737: - ssim 0.804469: \n","Epoch: 7 - 45 - mse 0.004002: - ssim 0.811349: \n","Epoch: 7 - 46 - mse 0.003538: - ssim 0.808286: \n","Epoch: 7 - 47 - mse 0.003809: - ssim 0.799643: \n","Epoch: 7 - 48 - mse 0.003871: - ssim 0.811073: \n","Epoch: 7 - 49 - mse 0.003224: - ssim 0.815303: \n","Epoch: 8 - 0 - mse 0.003456: - ssim 0.818641: \n","Epoch: 8 - 1 - mse 0.003755: - ssim 0.809900: \n","Epoch: 8 - 2 - mse 0.003627: - ssim 0.820650: \n","Epoch: 8 - 3 - mse 0.004136: - ssim 0.803814: \n","Epoch: 8 - 4 - mse 0.003191: - ssim 0.822601: \n","Epoch: 8 - 5 - mse 0.003378: - ssim 0.810455: \n","Epoch: 8 - 6 - mse 0.003978: - ssim 0.817306: \n","Epoch: 8 - 7 - mse 0.003375: - ssim 0.810749: \n","Epoch: 8 - 8 - mse 0.003755: - ssim 0.802896: \n","Epoch: 8 - 9 - mse 0.003100: - ssim 0.823042: \n","Epoch: 8 - 10 - mse 0.004231: - ssim 0.805539: \n","Epoch: 8 - 11 - mse 0.003520: - ssim 0.819005: \n","Epoch: 8 - 12 - mse 0.003213: - ssim 0.822856: \n","Epoch: 8 - 13 - mse 0.003329: - ssim 0.827521: \n","Epoch: 8 - 14 - mse 0.003016: - ssim 0.824389: \n","Epoch: 8 - 15 - mse 0.003273: - ssim 0.816388: \n","Epoch: 8 - 16 - mse 0.003458: - ssim 0.815716: \n","Epoch: 8 - 17 - mse 0.003366: - ssim 0.819181: \n","Epoch: 8 - 18 - mse 0.004196: - ssim 0.811870: \n","Epoch: 8 - 19 - mse 0.003746: - ssim 0.812783: \n","Epoch: 8 - 20 - mse 0.003463: - ssim 0.815256: \n","Epoch: 8 - 21 - mse 0.003496: - ssim 0.824243: \n","Epoch: 8 - 22 - mse 0.004046: - ssim 0.801961: \n","Epoch: 8 - 23 - mse 0.003013: - ssim 0.824176: \n","Epoch: 8 - 24 - mse 0.003854: - ssim 0.804839: \n","Epoch: 8 - 25 - mse 0.003128: - ssim 0.826658: \n","Epoch: 8 - 26 - mse 0.003746: - ssim 0.806642: \n","Epoch: 8 - 27 - mse 0.003548: - ssim 0.811076: \n","Epoch: 8 - 28 - mse 0.003347: - ssim 0.814349: \n","Epoch: 8 - 29 - mse 0.004065: - ssim 0.806795: \n","Epoch: 8 - 30 - mse 0.003877: - ssim 0.810028: \n","Epoch: 8 - 31 - mse 0.003402: - ssim 0.820143: \n","Epoch: 8 - 32 - mse 0.003037: - ssim 0.818098: \n","Epoch: 8 - 33 - mse 0.003469: - ssim 0.822272: \n","Epoch: 8 - 34 - mse 0.003568: - ssim 0.823117: \n","Epoch: 8 - 35 - mse 0.003572: - ssim 0.818439: \n","Epoch: 8 - 36 - mse 0.003470: - ssim 0.814680: \n","Epoch: 8 - 37 - mse 0.003460: - ssim 0.821493: \n","Epoch: 8 - 38 - mse 0.003519: - ssim 0.817753: \n","Epoch: 8 - 39 - mse 0.003270: - ssim 0.824381: \n","Epoch: 8 - 40 - mse 0.003161: - ssim 0.829486: \n","Epoch: 8 - 41 - mse 0.003427: - ssim 0.829063: \n","Epoch: 8 - 42 - mse 0.003448: - ssim 0.818971: \n","Epoch: 8 - 43 - mse 0.003220: - ssim 0.826281: \n","Epoch: 8 - 44 - mse 0.003549: - ssim 0.820444: \n","Epoch: 8 - 45 - mse 0.003471: - ssim 0.819461: \n","Epoch: 8 - 46 - mse 0.003278: - ssim 0.813437: \n","Epoch: 8 - 47 - mse 0.003518: - ssim 0.808920: \n","Epoch: 8 - 48 - mse 0.003570: - ssim 0.826902: \n","Epoch: 8 - 49 - mse 0.003677: - ssim 0.811835: \n","Epoch: 9 - 0 - mse 0.003771: - ssim 0.815765: \n","Epoch: 9 - 1 - mse 0.002970: - ssim 0.829436: \n","Epoch: 9 - 2 - mse 0.003207: - ssim 0.827182: \n","Epoch: 9 - 3 - mse 0.003507: - ssim 0.818838: \n","Epoch: 9 - 4 - mse 0.003082: - ssim 0.830092: \n","Epoch: 9 - 5 - mse 0.003332: - ssim 0.825740: \n","Epoch: 9 - 6 - mse 0.003195: - ssim 0.823523: \n","Epoch: 9 - 7 - mse 0.003492: - ssim 0.817191: \n","Epoch: 9 - 8 - mse 0.003552: - ssim 0.826588: \n","Epoch: 9 - 9 - mse 0.003097: - ssim 0.824295: \n","Epoch: 9 - 10 - mse 0.003366: - ssim 0.829847: \n","Epoch: 9 - 11 - mse 0.003778: - ssim 0.817330: \n","Epoch: 9 - 12 - mse 0.002882: - ssim 0.831673: \n","Epoch: 9 - 13 - mse 0.003267: - ssim 0.820477: \n","Epoch: 9 - 14 - mse 0.003350: - ssim 0.829781: \n","Epoch: 9 - 15 - mse 0.003697: - ssim 0.818421: \n","Epoch: 9 - 16 - mse 0.003412: - ssim 0.819287: \n","Epoch: 9 - 17 - mse 0.003244: - ssim 0.822732: \n","Epoch: 9 - 18 - mse 0.003914: - ssim 0.818407: \n","Epoch: 9 - 19 - mse 0.003488: - ssim 0.827762: \n","Epoch: 9 - 20 - mse 0.002933: - ssim 0.829317: \n","Epoch: 9 - 21 - mse 0.003316: - ssim 0.826465: \n","Epoch: 9 - 22 - mse 0.003206: - ssim 0.832157: \n","Epoch: 9 - 23 - mse 0.003343: - ssim 0.832238: \n","Epoch: 9 - 24 - mse 0.003319: - ssim 0.831070: \n","Epoch: 9 - 25 - mse 0.003266: - ssim 0.819722: \n","Epoch: 9 - 26 - mse 0.003129: - ssim 0.817231: \n","Epoch: 9 - 27 - mse 0.002849: - ssim 0.836121: \n","Epoch: 9 - 28 - mse 0.003584: - ssim 0.819169: \n","Epoch: 9 - 29 - mse 0.003020: - ssim 0.830221: \n","Epoch: 9 - 30 - mse 0.003097: - ssim 0.825494: \n","Epoch: 9 - 31 - mse 0.003251: - ssim 0.835299: \n","Epoch: 9 - 32 - mse 0.003863: - ssim 0.814032: \n","Epoch: 9 - 33 - mse 0.003118: - ssim 0.829023: \n","Epoch: 9 - 34 - mse 0.003246: - ssim 0.831531: \n","Epoch: 9 - 35 - mse 0.003395: - ssim 0.834054: \n","Epoch: 9 - 36 - mse 0.003559: - ssim 0.816101: \n","Epoch: 9 - 37 - mse 0.002793: - ssim 0.839209: \n","Epoch: 9 - 38 - mse 0.003959: - ssim 0.826589: \n","Epoch: 9 - 39 - mse 0.003526: - ssim 0.824678: \n","Epoch: 9 - 40 - mse 0.003517: - ssim 0.821656: \n","Epoch: 9 - 41 - mse 0.003426: - ssim 0.821530: \n","Epoch: 9 - 42 - mse 0.003103: - ssim 0.830461: \n","Epoch: 9 - 43 - mse 0.003322: - ssim 0.827584: \n","Epoch: 9 - 44 - mse 0.003192: - ssim 0.825839: \n","Epoch: 9 - 45 - mse 0.003405: - ssim 0.824924: \n","Epoch: 9 - 46 - mse 0.002880: - ssim 0.835344: \n","Epoch: 9 - 47 - mse 0.003039: - ssim 0.836856: \n","Epoch: 9 - 48 - mse 0.003307: - ssim 0.827012: \n","Epoch: 9 - 49 - mse 0.002951: - ssim 0.835668: \n","testing dataset\n","(500, 64, 64, 2)\n","0.003828416\n","0.002869657\n","0.0028613782\n","0.0028890437\n","0.002967667\n","0.0022581746\n","0.0020296741\n","0.0025739104\n","0.003613296\n","0.0033509885\n","0.0031002006\n","0.0035428167\n","0.003934641\n","0.0031836645\n","0.0032496126\n","0.003807276\n","0.0032592649\n","0.0030471482\n","0.0029012265\n","0.002863293\n","0.0029578456\n","0.0027872208\n","0.0029362857\n","0.0025521424\n","0.0028396\n","0.003027454\n","0.0031658933\n","0.0034863013\n","0.002816629\n","0.0030071868\n","0.0031837705\n","0.0029624163\n","0.002917923\n","0.0031173476\n","0.0031630404\n","0.0033630338\n","0.0022824171\n","0.002343121\n","0.0023489138\n","0.0025580174\n","0.0025903299\n","0.0027692365\n","0.002433911\n","0.002129328\n","0.0021009254\n","0.0023099603\n","0.0028503328\n","0.002528084\n","0.0026255804\n","0.002559917\n","0.00259916\n","0.0024529034\n","0.0023776987\n","0.0019752136\n","0.0019158024\n","0.0019167261\n","0.0018464397\n","0.0019362932\n","0.0024460747\n","0.0028968337\n","0.003161831\n","0.0033090003\n","0.0039732894\n","0.0047534015\n","0.0038626527\n","0.0037038147\n","0.0034543867\n","0.003398642\n","0.0031780805\n","0.0032946884\n","0.0029932437\n","0.0029238088\n","0.0025112235\n","0.002512634\n","0.0028417257\n","0.003373193\n","0.0036017485\n","0.0031652292\n","0.0032356272\n","0.003756761\n","0.0027479944\n","0.0029590642\n","0.0034166388\n","0.0029180143\n","0.003037883\n","0.0051811934\n","0.0034813073\n","0.0029944533\n","0.0032830294\n","0.0028520827\n","0.002465969\n","0.0022969963\n","0.00235395\n","0.0025162613\n","0.002546926\n","0.0025221175\n","0.0026203035\n","0.0022854148\n","0.002334704\n","0.002603895\n","0.0026424439\n","0.0026731864\n","0.0025730156\n","0.002640823\n","0.0028621096\n","0.002719907\n","0.0028086496\n","0.002797636\n","0.0028755106\n","0.0032121397\n","0.0032435325\n","0.0034097708\n","0.0034779394\n","0.0033846784\n","0.003618875\n","0.0035086982\n","0.0035155332\n","0.003358878\n","0.0034522736\n","0.0030351647\n","0.0030084257\n","0.0029391334\n","0.002591895\n","0.0026209266\n","0.0030528386\n","0.0032628784\n","0.0040282533\n","0.004150085\n","0.003858922\n","0.0033569678\n","0.0035518273\n","0.0038061053\n","0.003668348\n","0.003752877\n","0.003662501\n","0.003698808\n","0.003873765\n","0.0038826265\n","0.0039426554\n","0.004215275\n","0.0039947378\n","0.003960781\n","0.0043004565\n","0.0041072434\n","0.0042601274\n","0.0039874106\n","0.004377964\n","0.003914267\n","0.0040331245\n","0.0032533812\n","0.00289378\n","0.0028575736\n","0.0030424967\n","0.0032914127\n","0.0027617253\n","0.0028621892\n","0.0035250639\n","0.0036053588\n","0.003040316\n","0.0029220057\n","0.0032633634\n","0.00387564\n","0.0037787464\n","0.0030944515\n","0.0031991773\n","0.0031061524\n","0.003881576\n","0.0033266987\n","0.0030360979\n","0.0038416483\n","0.0024971126\n","0.0023328026\n","0.002231379\n","0.0023513206\n","0.002368412\n","0.0019383553\n","0.002519791\n","0.0026331437\n","0.0030897066\n","0.0032336994\n","0.0030335293\n","0.003163374\n","0.0030323924\n","0.0028479304\n","0.0029195028\n","0.0028655352\n","0.0029268318\n","0.0030423116\n","0.0030945318\n","0.0028610164\n","0.0030260962\n","0.0027913246\n","0.0027004806\n","0.002696829\n","0.002421504\n","0.0025979083\n","0.0029760767\n","0.0027740502\n","0.002593785\n","0.0028284723\n","0.0032356302\n","0.0023945267\n","0.0022732061\n","0.0024385229\n","0.002812035\n","0.0026603872\n","0.0028064256\n","0.0035892455\n","0.0028406463\n","0.0025129605\n","0.002584362\n","0.0025701344\n","0.0026023677\n","0.0025597555\n","0.0026801173\n","0.0026356797\n","0.0026168518\n","0.002887322\n","0.003048398\n","0.0033095009\n","0.003264268\n","0.0031010616\n","0.0029358184\n","0.0028649266\n","0.0029006419\n","0.0031040127\n","0.0027151958\n","0.0027475222\n","0.0024870783\n","0.0028213286\n","0.0030576224\n","0.0026617649\n","0.0027163536\n","0.0027473518\n","0.002986561\n","0.0037121647\n","0.002826198\n","0.0028585568\n","0.0030855427\n","0.0035900841\n","0.0039488003\n","0.0041462174\n","0.0042699645\n","0.0041490495\n","0.004283606\n","0.0037709675\n","0.0034594487\n","0.003839714\n","0.0038701398\n","0.004387499\n","0.0041586915\n","0.004599806\n","0.00426428\n","0.0042902753\n","0.005079381\n","0.0037342622\n","0.003585169\n","0.0032206946\n","0.0031079876\n","0.0034569823\n","0.0029754862\n","0.0028132934\n","0.0028618218\n","0.002973468\n","0.0034512866\n","0.0036480029\n","0.003378574\n","0.003256395\n","0.003969118\n","0.003562719\n","0.0031972863\n","0.0034249783\n","0.003193859\n","0.0031801572\n","0.0029623797\n","0.002993674\n","0.0032319226\n","0.00389477\n","0.00347591\n","0.0030325393\n","0.0033712585\n","0.0032506953\n","0.0030098096\n","0.0028730156\n","0.002751949\n","0.0030022697\n","0.0032741185\n","0.0033381043\n","0.0033660633\n","0.003381257\n","0.0036525186\n","0.0038422975\n","0.0040850113\n","0.0044284854\n","0.004009054\n","0.0036384223\n","0.0034992727\n","0.003477474\n","0.0035031294\n","0.0034428416\n","0.003827397\n","0.0035897964\n","0.0036110121\n","0.0041660527\n","0.004049612\n","0.0039439355\n","0.003882928\n","0.0037669113\n","0.0037867064\n","0.0037580852\n","0.0035480512\n","0.003659134\n","0.0041276133\n","0.005398096\n","0.0050109816\n","0.0047770203\n","0.0050618057\n","0.004819504\n","0.0051216027\n","0.0064530903\n","0.005805418\n","0.0059205573\n","0.006039624\n","0.0057640253\n","0.006003656\n","0.005976271\n","0.005367863\n","0.0056545953\n","0.0067085475\n","0.0067286617\n","0.0060169543\n","0.005926852\n","0.006124857\n","0.005664455\n","0.0057917875\n","0.006899623\n","0.006612122\n","0.005229273\n","0.005019638\n","0.005069786\n","0.0044870684\n","0.004146545\n","0.0043032076\n","0.0036443952\n","0.0035912846\n","0.0024528364\n","0.0024765078\n","0.002237828\n","0.0025024211\n","0.0027503357\n","0.0023526545\n","0.002687063\n","0.0022726485\n","0.0021921948\n","0.00240647\n","0.0028627678\n","0.0027936718\n","0.0028974484\n","0.0028414002\n","0.0027462183\n","0.0029661488\n","0.0026987337\n","0.0025396447\n","0.0025558486\n","0.0029219093\n","0.002744297\n","0.0027997578\n","0.0028621864\n","0.002730117\n","0.0030361982\n","0.0029365798\n","0.0030068588\n","0.003049145\n","0.0024870208\n","0.0024553402\n","0.0029907532\n","0.002575309\n","0.0019179619\n","0.0018884063\n","0.002647832\n","0.0025127213\n","0.0026916512\n","0.002505255\n","0.0022344892\n","0.0022342154\n","0.0023484726\n","0.0024795032\n","0.0025722026\n","0.0024060146\n","0.0023503432\n","0.0023781443\n","0.0025126957\n","0.002159201\n","0.0021613147\n","0.0026234295\n","0.0028981736\n","0.0025614467\n","0.0032710074\n","0.0025676677\n","0.0022869315\n","0.0021841116\n","0.0020696623\n","0.0021233521\n","0.002701297\n","0.0026916945\n","0.0030055821\n","0.0028520485\n","0.0025636004\n","0.0029492879\n","0.0032582807\n","0.0031832217\n","0.0033045015\n","0.0031912208\n","0.0031213835\n","0.0028113665\n","0.002905731\n","0.0030832328\n","0.0030340706\n","0.0035757567\n","0.003587427\n","0.0041315556\n","0.0033102005\n","0.002970939\n","0.0027890934\n","0.0033423733\n","0.0025714182\n","0.0023184086\n","0.002285611\n","0.0023770235\n","0.0021101502\n","0.0019188671\n","0.0020927803\n","0.002048043\n","0.00203858\n","0.0025000335\n","0.0026560787\n","0.0029437458\n","0.0027499865\n","0.0025213643\n","0.0023838943\n","0.002457798\n","0.0027599516\n","0.0028338209\n","0.0027232822\n","0.0028128545\n","0.0027135615\n","0.002739246\n","0.0026329772\n","0.0026602237\n","0.0026787408\n","0.0029422014\n","0.0031109862\n","0.0032507188\n","0.0029359288\n","0.0025279368\n","0.0026651388\n","0.0030784616\n","0.0030232666\n","0.002615476\n","0.002408391\n","0.0022384678\n","0.0021289818\n","0.0023141939\n","0.0024550874\n","0.0025708275\n","0.0023924615\n","0.0025239545\n","0.0025805226\n","0.0026501054\n","0.0026081307\n","0.0026176218\n","0.0021541505\n","0.0020460514\n","0.002206818\n","0.0022370233\n","0.0022434941\n","0.0025518439\n","0.002307715\n","0.00248393\n","0.0024554676\n","0.0024289289\n","0.0022089235\n","0.0023739915\n","0.0024991266\n","0.0024910383\n","0.0025424915\n","0.0026434334\n","0.0024311258\n","0.0025096452\n","0.0032289291\n","0.0034751734\n","0.0032644824\n","0.0029300416\n","0.0027939314\n","0.0029372815\n","0.00281042\n","0.0030033195\n","0.0038190542\n","0.0037329725\n","0.004051663\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1f479fd2076f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator_model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mestimated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfake_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_batch_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0mt_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (0, 64, 64, 2) for Tensor 'fake:0', which has shape '(10, 64, 64, 2)'"]}]}]}