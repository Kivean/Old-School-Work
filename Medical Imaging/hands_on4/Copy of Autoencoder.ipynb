{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Autoencoder.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yxmW_YF8UDvK","colab_type":"code","outputId":"e9fe3802-764f-477d-9ac2-3196b90e3080","executionInfo":{"status":"ok","timestamp":1573017299118,"user_tz":300,"elapsed":1163,"user":{"displayName":"Grayson Gerlich","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBNxDB08OJW4-p4q40fCqc7j2U0aBxlJ-QxoIw78g=s64","userId":"09958448879325523259"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#!/usr/bin/env python\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import time\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","from skimage.transform import radon, iradon\n","from skimage.measure import compare_ssim\n","import math\n","from google.colab import drive    # in ordre to mount drive to colab\n","drive.mount('/content/gdrive')\n","\n","gpu_options = tf.GPUOptions(allow_growth=True)\n","session_config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options, allow_soft_placement=True)\n","sess = tf.Session(config=session_config)\n","\n","batch_size = 32\n","learning_rate = 1.0e-4\n","width = 256\n","patch_width = 64\n","lambda_sl = 0.5\n","use_gan = True # change to False if not using GAN (LS_GAN).\n","if use_gan:\n","  disc_iters = 4\n","  lambda_al = 0.0025\n","else:\n","  disc_iters = 0\n","  lambda_al = 0\n","\n","num_epoch = 10"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GYR_-E3YXx9t","colab_type":"code","colab":{}},"source":["def read_data(file_name):\n","    f = h5py.File(file_name, 'r')         # read dataset\n","    label = np.array(f['data'])           \n","    f.close()\n","    return label                 # return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FExTTTTpIFGA","colab_type":"code","colab":{}},"source":["def two_dim_conv(batch, name, n_filter):\n","    c = tf.layers.conv2d(\n","        inputs=batch,\n","        filters=n_filter,\n","        kernel_size=[3, 3],\n","        strides=1,\n","        padding='same',\n","        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","        use_bias=True,\n","        activation=None,\n","        name=name,\n","        reuse=tf.AUTO_REUSE\n","    )\n","    #print(\"{}:{}\".format(name, c.get_shape()))\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"07iWL7aVIIu4","colab_type":"code","colab":{}},"source":["def U_res_block_layer(batch, name, n_out):\n","    conv_1 = two_dim_conv(batch, name + 'res_conv1', n_out)\n","    conv_1 = tf.nn.relu(conv_1)\n","\n","    conv_1 = tf.concat([batch, conv_1], -1)\n","    conv_2 = two_dim_conv(conv_1, name + 'res_conv2', n_out)\n","    conv_2 = tf.nn.relu(conv_2)\n","\n","    conv_2 = tf.concat([batch, conv_1, conv_2], -1)\n","    conv_3 = two_dim_conv(conv_2, name + 'res_conv3', n_out)\n","    block_res = tf.nn.relu(conv_3)\n","\n","    return block_res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7meU2WOUWekI","colab_type":"code","colab":{}},"source":["def generator(x, padding='Valid'):\n","    outputs1 = tf.layers.conv2d(x, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1', use_bias=True)\n","    outputs2 = tf.nn.relu(outputs1)\n","    # 2-D convolutional layers\n","    outputs2 = U_res_block_layer(outputs2, \"res1\", 32)\n","    \n","\n","    outputs2 = tf.layers.conv2d(outputs2, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2', use_bias=False)\n","    outputs3 = tf.nn.relu(outputs2)\n","    outputs3 = U_res_block_layer(outputs3, \"res2\", 32)\n","\n","    outputs3 = tf.layers.conv2d(outputs3, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv3', use_bias=False)\n","    outputs4 = tf.nn.relu(outputs3)\n","    outputs4 = U_res_block_layer(outputs4, \"res3\", 32)\n","\n","    outputs4 = tf.layers.conv2d(outputs4, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv4', use_bias=False)\n","    outputs5 = tf.nn.relu(outputs4)\n","    outputs5 = U_res_block_layer(outputs5, \"res4\", 32)\n","    \n","    outputs5 = tf.layers.conv2d_transpose(outputs5, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv5', \n","                                          use_bias=False)\n","    # transpose 2-D convolutional layers\n","    outputs5 = tf.nn.relu(outputs5)\n","    outputs5 = U_res_block_layer(outputs5, \"res5\", 32)\n","    outputs5 = tf.concat([outputs3, outputs5], -1)\n","\n","    outputs6 = tf.layers.conv2d_transpose(outputs5, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv6', use_bias=False)\n","    outputs6 = tf.nn.relu(outputs6)\n","    outputs6 = U_res_block_layer(outputs6, \"res6\", 32)    \n","    outputs6 = tf.concat([outputs2, outputs6], -1)\n","\n","    outputs7 = tf.layers.conv2d_transpose(outputs6, 32, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv7', use_bias=False)\n","    outputs7 = tf.nn.relu(outputs7)\n","    outputs7 = U_res_block_layer(outputs7, \"res7\", 32)\n","    outputs7 = tf.concat([outputs1, outputs7], -1)\n","\n","    outputs8 = tf.layers.conv2d_transpose(outputs7, 1, 3, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='deconv8', use_bias=False)\n","    outputs = tf.nn.relu(outputs8)\n","    outputs = U_res_block_layer(outputs, \"res8\", 1)\n","    \n","    return outputs \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1N-ujiOfzK-","colab_type":"code","colab":{}},"source":["def discriminator(x):  \n","    outputs = tf.layers.conv2d(x, 64, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1')\n","    outputs = tf.nn.relu(outputs)\n","        \n","    outputs = tf.layers.conv2d(outputs, 64, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv3')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 128, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv4')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv5')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.conv2d(outputs, 256, 3, padding='SAME', strides=(2,2), kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv6')\n","    outputs = tf.nn.relu(outputs)\n","    \n","    outputs = tf.layers.dense(outputs, units=1024, name='dense1')\n","    outputs = tf.nn.relu(outputs)\n","    outputs = tf.layers.dense(outputs, units=1, name='dense2')\n","    return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqHrZIWzWy93","colab_type":"code","outputId":"97e54379-24d0-47e5-9224-234e56d65761","executionInfo":{"status":"error","timestamp":1573017459437,"user_tz":300,"elapsed":2266,"user":{"displayName":"Grayson Gerlich","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBNxDB08OJW4-p4q40fCqc7j2U0aBxlJ-QxoIw78g=s64","userId":"09958448879325523259"}},"colab":{"base_uri":"https://localhost:8080/","height":581}},"source":["lr = tf.placeholder(dtype=tf.float32, shape=[])  # learning rate\n","real_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, patch_width, patch_width, 1], name='real')        #real images\n","fake_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, patch_width, patch_width, 1], name='fake')        #fake images\n","fake_placeholder_test = tf.placeholder(dtype=tf.float32, shape=[1, width, width, 1], name='fake_test')                   #fakeing tesing images\n","with tf.variable_scope('generator_model', reuse=tf.AUTO_REUSE) as scope_generator_model:\n","    Gz = generator(fake_placeholder)                 # feed patches for training\n","    Gz_test = generator(fake_placeholder_test)       # feed whole images for testing\n","\n","with tf.variable_scope('discriminator_model', reuse=tf.AUTO_REUSE) as scope_discriminator_model:\n","    FD_d = discriminator(real_placeholder)           # feed real images into discriminator network\n","    LD_d = discriminator(Gz)                         # feed fake images into discriminator network\n","\n","\n","disc_loss = tf.reduce_mean((FD_d - 1)**2 + (LD_d - 0)**2)       # loss function for LS-GAN]\n","disc_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator_model')\n","d_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(disc_loss, var_list=disc_variables) # training discriminator\n","\n","ssim = tf.squeeze(tf.reduce_mean(tf.image.ssim(Gz, real_placeholder, 1.0)))\n","ssim_loss = 1 - ssim\n","mse = tf.reduce_mean(tf.squared_difference(Gz, real_placeholder))\n","gen_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_model')\n","gen_cost = tf.reduce_mean((LD_d - 1)**2)          # GAN loss for generator\n","\n","g_trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(mse + lambda_al * gen_cost + lambda_sl * ssim_loss, var_list=gen_variables) # training generator\n","\n","saver = tf.train.Saver()\n","print(\"enter Session\")\n","sess.run(tf.global_variables_initializer())       # initialize parameters\n","#saver.restore(sess, '/content/gdrive/My Drive/hands_on4/Autoencoder/SIGmathmodel100.ckpt')  # use to restore model\n","print('model restored')\n","print(\"Begin training\")\n","label = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Train/Label.hdf5') # read label\n","data = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Train/Data.hdf5') # read data\n","\n","\n","for iteration in range(num_epoch):\n","    #break\n","    label, data = shuffle(label, data)  # shuffling data & label, same as last time\n","    val_lr = learning_rate / (iteration + 1)       # learning rate is decreased\n","    num_batches = data.shape[0] // batch_size      # how many batches?\n","    for i in range(num_batches):\n","        for _ in range(disc_iters): # disc_iters will be set as 0 if you de-activate GAN\n","            idx = np.random.permutation(label.shape[0])       # the input to the discriminator and the input to the generator should be different\n","            batch_label = label[idx[:batch_size]]\n","            batch_label = np.expand_dims(batch_label, -1)\n","            \n","            batch_data = data[idx[:batch_size]]\n","            batch_data = np.expand_dims(batch_data, -1)\n","            \n","            _ = sess.run(d_trainer, feed_dict={real_placeholder: batch_label, # training discriminator\n","                                               fake_placeholder: batch_data,\n","                                               lr: val_lr}) \n","            \n","        batch_label = label[i * batch_size: (i + 1) * batch_size] # the input to the discriminator and the input to the generator should be different\n","        batch_label = np.expand_dims(batch_label, -1)\n","        batch_data = data[i * batch_size: (i + 1) * batch_size]\n","        batch_data = np.expand_dims(batch_data, -1)\n","        _mse, _ssim, _ = sess.run([mse, ssim, g_trainer], feed_dict={real_placeholder: (batch_label),  # training generator\n","                                                                             fake_placeholder: (batch_data), lr: val_lr})\n","\n","        print('Epoch: %d - %d - mse %.6f: - ssim %.6f: ' % (iteration, i, _mse, _ssim))\n","\n","saver.save(sess, '/content/gdrive/My Drive/hands_on4/Autoencoder/SIGmathmodel100.ckpt')  # save model\n","  \n","print(\"testing dataset\")\n","test_label = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Test/Label.hdf5')   # reading testing label\n","test_data = read_data('/content/gdrive/My Drive/hands_on4/Autoencoder/Test/Data.hdf5')   # reaing testing data\n","for c in range(test_label.shape[0]):\n","    test_batch_label = test_label[c * 1: (c + 1) * 1]  # feed images one by one\n","    test_batch_label = np.expand_dims(test_batch_label, -1)\n","    \n","    test_batch_data = test_data[c * 1: (c + 1) * 1]\n","    test_batch_data = np.expand_dims(test_batch_data, -1)\n","    \n","    with tf.variable_scope('generator_model') as scope:\n","        scope.reuse_variables()\n","        estimated = sess.run(Gz_test, feed_dict={fake_placeholder_test: test_batch_data})      # testing\n","        t_r = np.squeeze(test_batch_label)\n","        e_1 = np.squeeze(estimated)\n","        t_o = np.squeeze(test_batch_data)\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'validation.jpg' #save images\n","        plt.imsave(f_n, e_1, cmap='gray')\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'real_.jpg'  # save images\n","        plt.imsave(f_n, t_r, cmap='gray')\n","        f_n = '/content/gdrive/My Drive/hands_on4/Autoencoder/testing_results/' + str(c) + 'original_.jpg'\n","        plt.imsave(f_n, t_o, cmap='gray')\n","\n","        z = (t_r - e_1) ** 2\n","        print(np.mean(z))   # print mse loss."],"execution_count":19,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-857d160354e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFD_d\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLD_d\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# loss function for LS-GAN]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdisc_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discriminator_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0md_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisc_variables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# training discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 413\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   def compute_gradients(self, loss, var_list=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    595\u001b[0m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m       \u001b[0mnew_slot_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m       self._restore_slot_variable(\n\u001b[1;32m   1158\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    188\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    189\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 164\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     75\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Variable discriminator_model/conv1/kernel/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"]}]}]}