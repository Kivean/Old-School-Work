step 0, training loss: 0.100177
step 500, training loss: 0.062007
step 1000, training loss: 0.0600998
step 1500, training loss: 0.0574519
step 2000, training loss: 0.0602227
step 2500, training loss: 0.0551379
step 3000, training loss: 0.0592607
step 3500, training loss: 0.0542045
step 4000, training loss: 0.0551902
step 4500, training loss: 0.054862
step 5000, training loss: 0.0571953
step 5500, training loss: 0.0577121
step 6000, training loss: 0.0568497
step 6500, training loss: 0.0494593
step 7000, training loss: 0.0526669
step 7500, training loss: 0.0518093
step 8000, training loss: 0.0548083
step 8500, training loss: 0.0538171
step 9000, training loss: 0.0546663
step 9500, training loss: 0.0544272
step 10000, training loss: 0.0491561
step 10500, training loss: 0.050754
step 11000, training loss: 0.0565375
step 11500, training loss: 0.0540042
step 12000, training loss: 0.0544453
step 12500, training loss: 0.0561083
step 13000, training loss: 0.0518207
step 13500, training loss: 0.0487776
step 14000, training loss: 0.0536955
step 14500, training loss: 0.0545303
step 15000, training loss: 0.0512707
step 15500, training loss: 0.0502549
step 16000, training loss: 0.04891
step 16500, training loss: 0.0483923
step 17000, training loss: 0.0481869
step 17500, training loss: 0.0459302
step 18000, training loss: 0.0535188
step 18500, training loss: 0.0450791
step 19000, training loss: 0.041393
step 19500, training loss: 0.0484634
step 20000, training loss: 0.0479633
step 20500, training loss: 0.0482247
step 21000, training loss: 0.0466751
step 21500, training loss: 0.0450641
step 22000, training loss: 0.0500024
step 22500, training loss: 0.0495203
step 23000, training loss: 0.0476563
step 23500, training loss: 0.0481059
step 24000, training loss: 0.0440492
step 24500, training loss: 0.0434797
step 25000, training loss: 0.045226
step 25500, training loss: 0.0421118
step 26000, training loss: 0.0457223
step 26500, training loss: 0.0429595
step 27000, training loss: 0.0455247
step 27500, training loss: 0.0408382
step 28000, training loss: 0.0445921
step 28500, training loss: 0.0484898
step 29000, training loss: 0.0451296
step 29500, training loss: 0.0445557
step 30000, training loss: 0.0453783
step 30500, training loss: 0.0456961
step 31000, training loss: 0.0450635
step 31500, training loss: 0.0425577
step 32000, training loss: 0.0404595
step 32500, training loss: 0.043329
step 33000, training loss: 0.0429863
step 33500, training loss: 0.0397587
step 34000, training loss: 0.0482428
step 34500, training loss: 0.0419815
step 35000, training loss: 0.0456763
step 35500, training loss: 0.0431482
step 36000, training loss: 0.0472417
step 36500, training loss: 0.0433775
step 37000, training loss: 0.0449442
step 37500, training loss: 0.0459012
step 38000, training loss: 0.0442617
step 38500, training loss: 0.0434559
step 39000, training loss: 0.0428922
step 39500, training loss: 0.0437228
step 40000, training loss: 0.0444796
step 40500, training loss: 0.0447105
step 41000, training loss: 0.0469735
step 41500, training loss: 0.0430762
step 42000, training loss: 0.0474483
step 42500, training loss: 0.042998
step 43000, training loss: 0.0470194
step 43500, training loss: 0.0418465
step 44000, training loss: 0.0464535
step 44500, training loss: 0.0428316
step 45000, training loss: 0.0423002
step 45500, training loss: 0.0354187
step 46000, training loss: 0.0444539
step 46500, training loss: 0.044417
step 47000, training loss: 0.0456011
step 47500, training loss: 0.0409224
step 48000, training loss: 0.0422818
step 48500, training loss: 0.0455137
step 49000, training loss: 0.043742
step 49500, training loss: 0.0434779
step 50000, training loss: 0.0413962
step 50500, training loss: 0.0386035
step 51000, training loss: 0.0440177
step 51500, training loss: 0.0419719
step 52000, training loss: 0.0452723
step 52500, training loss: 0.0466374
step 53000, training loss: 0.0421738
step 53500, training loss: 0.0425693
step 54000, training loss: 0.0474043
step 54500, training loss: 0.0432848
step 55000, training loss: 0.0408785
step 55500, training loss: 0.046578
step 56000, training loss: 0.0441086
step 56500, training loss: 0.0393981
step 57000, training loss: 0.045037
step 57500, training loss: 0.0468236
step 58000, training loss: 0.0419872
step 58500, training loss: 0.041506
step 59000, training loss: 0.0459666
step 59500, training loss: 0.0391138
step 60000, training loss: 0.0416473
step 60500, training loss: 0.0422363
step 61000, training loss: 0.0425476
step 61500, training loss: 0.0424136
step 62000, training loss: 0.0421316
step 62500, training loss: 0.0379129
step 63000, training loss: 0.0396987
step 63500, training loss: 0.0468835
step 64000, training loss: 0.0421793
step 64500, training loss: 0.0407382
step 65000, training loss: 0.0426165
step 65500, training loss: 0.0407818
step 66000, training loss: 0.0445856
step 66500, training loss: 0.0433392
step 67000, training loss: 0.0422417
step 67500, training loss: 0.042294
step 68000, training loss: 0.041504
step 68500, training loss: 0.0417434
step 69000, training loss: 0.0430317
step 69500, training loss: 0.037599
step 70000, training loss: 0.0436895
step 70500, training loss: 0.0384677
step 71000, training loss: 0.0421199
step 71500, training loss: 0.038807
step 72000, training loss: 0.0439075
step 72500, training loss: 0.0410296
step 73000, training loss: 0.0441041
step 73500, training loss: 0.0386095
step 74000, training loss: 0.0412026
step 74500, training loss: 0.0399075
step 75000, training loss: 0.0434162
step 75500, training loss: 0.0408268
step 76000, training loss: 0.0420131
step 76500, training loss: 0.0402656
step 77000, training loss: 0.0409471
step 77500, training loss: 0.0431801
step 78000, training loss: 0.0387247
step 78500, training loss: 0.0421782
step 79000, training loss: 0.0425109
step 79500, training loss: 0.0401971
step 80000, training loss: 0.0382828
step 80500, training loss: 0.0445791
step 81000, training loss: 0.0422668
step 81500, training loss: 0.04444
step 82000, training loss: 0.0385323
step 82500, training loss: 0.0394474
step 83000, training loss: 0.0413852
step 83500, training loss: 0.0412476
step 84000, training loss: 0.0407661
step 84500, training loss: 0.0404576
step 85000, training loss: 0.0403676
step 85500, training loss: 0.0374125
step 86000, training loss: 0.044153
step 86500, training loss: 0.041961
step 87000, training loss: 0.0416497
step 87500, training loss: 0.0428608
step 88000, training loss: 0.0375855
step 88500, training loss: 0.044892
step 89000, training loss: 0.044759
step 89500, training loss: 0.038086
step 90000, training loss: 0.0435986
step 90500, training loss: 0.0419338
step 91000, training loss: 0.0420153
step 91500, training loss: 0.0399227
step 92000, training loss: 0.0443294
step 92500, training loss: 0.0400338
step 93000, training loss: 0.037758
step 93500, training loss: 0.0429147
step 94000, training loss: 0.0386317
step 94500, training loss: 0.0381681
step 95000, training loss: 0.0423087
step 95500, training loss: 0.0375507
step 96000, training loss: 0.0402639
step 96500, training loss: 0.0416984
step 97000, training loss: 0.0416132
step 97500, training loss: 0.0442816
step 98000, training loss: 0.0413645
step 98500, training loss: 0.0453892
step 99000, training loss: 0.0430112
step 99500, training loss: 0.0418034
step 100000, training loss: 0.0455117
step 100500, training loss: 0.0394152
step 101000, training loss: 0.0412008
step 101500, training loss: 0.0355802
step 102000, training loss: 0.0403797
step 102500, training loss: 0.0430845
step 103000, training loss: 0.0362706
step 103500, training loss: 0.0403051
step 104000, training loss: 0.0416215
step 104500, training loss: 0.0378751
step 105000, training loss: 0.0379767
step 105500, training loss: 0.0383198
step 106000, training loss: 0.0395762
step 106500, training loss: 0.0374683
step 107000, training loss: 0.040663
step 107500, training loss: 0.041855
step 108000, training loss: 0.041922
step 108500, training loss: 0.0415869
step 109000, training loss: 0.0427432
step 109500, training loss: 0.0382133
step 110000, training loss: 0.0373081
step 110500, training loss: 0.0434953
step 111000, training loss: 0.0406875
step 111500, training loss: 0.0403438
step 112000, training loss: 0.04303
step 112500, training loss: 0.0386053
step 113000, training loss: 0.0435414
step 113500, training loss: 0.0381354
step 114000, training loss: 0.0371022
step 114500, training loss: 0.0376112
step 115000, training loss: 0.0418132
step 115500, training loss: 0.0389806
step 116000, training loss: 0.0396104
step 116500, training loss: 0.0400572
step 117000, training loss: 0.0332489
step 117500, training loss: 0.0414629
step 118000, training loss: 0.0399757
step 118500, training loss: 0.037667
step 119000, training loss: 0.0426804
step 119500, training loss: 0.0410448
step 120000, training loss: 0.0410113
step 120500, training loss: 0.0408545
step 121000, training loss: 0.0400636
step 121500, training loss: 0.0426369
step 122000, training loss: 0.038944
step 122500, training loss: 0.0358235
step 123000, training loss: 0.0352662
step 123500, training loss: 0.0397912
step 124000, training loss: 0.0386454
step 124500, training loss: 0.0413033
step 125000, training loss: 0.0339316
step 125500, training loss: 0.039975
step 126000, training loss: 0.0393003
step 126500, training loss: 0.038194
step 127000, training loss: 0.0381659
step 127500, training loss: 0.0405531
step 128000, training loss: 0.0389524
step 128500, training loss: 0.0441333
step 129000, training loss: 0.0362942
step 129500, training loss: 0.038595
step 130000, training loss: 0.0403733
step 130500, training loss: 0.0383688
step 131000, training loss: 0.0435401
step 131500, training loss: 0.0400286
step 132000, training loss: 0.0389809
step 132500, training loss: 0.0373859
step 133000, training loss: 0.0450347
step 133500, training loss: 0.0413453
step 134000, training loss: 0.0411062
step 134500, training loss: 0.038735
step 135000, training loss: 0.0381301
step 135500, training loss: 0.0431451
step 136000, training loss: 0.0388998
step 136500, training loss: 0.0389313
step 137000, training loss: 0.0379525
step 137500, training loss: 0.0375626
step 138000, training loss: 0.0382315
step 138500, training loss: 0.0384812
step 139000, training loss: 0.0420423
step 139500, training loss: 0.039465
step 140000, training loss: 0.0386802
step 140500, training loss: 0.0362618
step 141000, training loss: 0.0411954
step 141500, training loss: 0.0389135
step 142000, training loss: 0.0380181
step 142500, training loss: 0.0403881
step 143000, training loss: 0.0412337
step 143500, training loss: 0.0375369
step 144000, training loss: 0.0363942
step 144500, training loss: 0.0421987
step 145000, training loss: 0.0379212
step 145500, training loss: 0.0412176
step 146000, training loss: 0.0402522
step 146500, training loss: 0.0360197
step 147000, training loss: 0.0415318
step 147500, training loss: 0.0415461
step 148000, training loss: 0.0408471
step 148500, training loss: 0.0382861
step 149000, training loss: 0.0400882
step 149500, training loss: 0.0445422
step 150000, training loss: 0.038883
step 150500, training loss: 0.0389767
step 151000, training loss: 0.0381522
step 151500, training loss: 0.0353675
step 152000, training loss: 0.0400245
step 152500, training loss: 0.0373909
step 153000, training loss: 0.0390459
step 153500, training loss: 0.0400626
step 154000, training loss: 0.0372517
step 154500, training loss: 0.0429748
step 155000, training loss: 0.036094
step 155500, training loss: 0.0409225
step 156000, training loss: 0.0361922
step 156500, training loss: 0.0381537
step 157000, training loss: 0.0392896
step 157500, training loss: 0.035787
step 158000, training loss: 0.0374313
step 158500, training loss: 0.0391711
step 159000, training loss: 0.0377853
step 159500, training loss: 0.0422023
step 160000, training loss: 0.0401789
step 160500, training loss: 0.0379053
step 161000, training loss: 0.0374835
step 161500, training loss: 0.038535
step 162000, training loss: 0.0415599
step 162500, training loss: 0.0384577
step 163000, training loss: 0.0402523
step 163500, training loss: 0.0381726
step 164000, training loss: 0.0388068
step 164500, training loss: 0.0367845
step 165000, training loss: 0.0388091
step 165500, training loss: 0.0398173
step 166000, training loss: 0.0381269
step 166500, training loss: 0.0381361
step 167000, training loss: 0.0420421
step 167500, training loss: 0.0407273
step 168000, training loss: 0.038278
step 168500, training loss: 0.0431471
step 169000, training loss: 0.0375794
step 169500, training loss: 0.037273
step 170000, training loss: 0.0402347
step 170500, training loss: 0.0360757
step 171000, training loss: 0.0407963
step 171500, training loss: 0.0392717
step 172000, training loss: 0.0399874
step 172500, training loss: 0.0376963
step 173000, training loss: 0.0355629
step 173500, training loss: 0.0387618
step 174000, training loss: 0.0376413
step 174500, training loss: 0.0400678
step 175000, training loss: 0.0395569
step 175500, training loss: 0.0362477
step 176000, training loss: 0.0375334
step 176500, training loss: 0.0384933
step 177000, training loss: 0.034699
step 177500, training loss: 0.0364932
step 178000, training loss: 0.037377
step 178500, training loss: 0.037477
step 179000, training loss: 0.0405125
step 179500, training loss: 0.0393729
step 180000, training loss: 0.0365783
step 180500, training loss: 0.0410329
step 181000, training loss: 0.0400615
step 181500, training loss: 0.0338821
step 182000, training loss: 0.0351348
step 182500, training loss: 0.0383578
step 183000, training loss: 0.0398423
step 183500, training loss: 0.0383126
step 184000, training loss: 0.0337787
step 184500, training loss: 0.0389658
step 185000, training loss: 0.0420223
step 185500, training loss: 0.0390914
step 186000, training loss: 0.0343169
step 186500, training loss: 0.0413603
step 187000, training loss: 0.0340923
step 187500, training loss: 0.0424397
step 188000, training loss: 0.0375714
step 188500, training loss: 0.0354372
step 189000, training loss: 0.0374046
step 189500, training loss: 0.0381412
step 190000, training loss: 0.0377414
step 190500, training loss: 0.0388962
step 191000, training loss: 0.0382823
step 191500, training loss: 0.0405324
step 192000, training loss: 0.04001
step 192500, training loss: 0.0321499
step 193000, training loss: 0.0369184
step 193500, training loss: 0.0342098
step 194000, training loss: 0.0346202
step 194500, training loss: 0.0371707
step 195000, training loss: 0.0414499
step 195500, training loss: 0.0365972
step 196000, training loss: 0.0363411
step 196500, training loss: 0.0391608
step 197000, training loss: 0.0397501
step 197500, training loss: 0.0390643
step 198000, training loss: 0.0374536
step 198500, training loss: 0.0404004
step 199000, training loss: 0.0354937
step 199500, training loss: 0.0359342
step 200000, training loss: 0.0369763